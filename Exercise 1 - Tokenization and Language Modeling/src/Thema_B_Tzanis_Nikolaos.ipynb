{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ερώτημα Β: N-gram Language Models\n",
    "Σε αυτή την άσκηση χρησιμοποιούμε ορισμένα κείμενα και πάλι από το Wall Street Journal. Τα κείμενα αυτά παρέχονται από τη βιβλιοθήκη nltk.corpus και στις προτάσεις τους έχει γίνει ήδη tokenization. Με τα κείμενα αυτά θα δημιουργήσουμε διάφορα μοντέλα διγραμμάτων ή τριγραμμάτων, θα κάνουμε k-smoothing και θα υπολογίσουμε το perplexity σε κάθε περίπτωση για να τα συγκρίνουμε μεταξύ τους."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Απαραίτητα Libraries\n",
    "Ορισμένα από τα libraries που θα χρησιμοποιήσουμε είναι το NumPy το οποίο χρειάζεται για διάφορους αριθμητικούς υπολογισμούς και αρκετά libraries του nltk που χρησιμοποιούνται για τη δημιουργία των ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import treebank\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ngrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η βιβλιοθήκη περιέχει 199 files. Σύμφωνα με την εκφώνηση τα πρώτα 170 files πρέπει να χρησιμοποιηθούν για το training των μοντέλων μας και τα υπόλοιπα 29 για το testing. Επομένως χωρίζουμε τα δεδομένα με αυτό τον τρόπο δημιουργώντας το training set και το test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = treebank.fileids()\n",
    "\n",
    "training_set_sentences = []\n",
    "for file in files[:170]:\n",
    "    for sentence in treebank.sents(file):\n",
    "        training_set_sentences.append(sentence)\n",
    "\n",
    "test_set_sentences = []\n",
    "for file in files[170:]:\n",
    "    for sentence in treebank.sents(file):\n",
    "        test_set_sentences.append(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Δημιουργία του Vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά ενώνω όλες τις προτάσεις του training set μεταξύ τους δημιουργώντας μία μεγάλη λίστα από tokens. Με τον τρόπο αυτό θα μπορέσω αργότερα να εξετάσω με ποια συχνότητα εμφανίζεται το κάθε token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens = []\n",
    "for sentence in training_set_sentences:\n",
    "    training_tokens.extend(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργώ ένα dictionary το οποίο έχει ως keys τα tokens και ως values τον αριθμό των εμφανίσεών τους στο training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, counts = np.unique(training_tokens, return_counts=True) \n",
    "tokens_dict = dict(zip(tokens, counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια, σύμφωνα με την εκφώνηση, δημιουργώ το Vocabulary αφαιρώντας όλα τα tokens που εμφανίζονται κάτω από 3 φορές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '*', '*-1', '*-2', '*-25', '*-3', '*-4', '*-5', '*-52', '*-6', '*-64', '*-7', '*-73', '*-80', '*?*', '*EXP*-1', '*EXP*-2', '*EXP*-3', '*ICH*-1', '*ICH*-2', '*ICH*-3', '*ICH*-4', '*PPA*-3', '*RNR*-1', '*RNR*-2', '*T*-1', '*T*-2', '*T*-3', '*T*-4', '*T*-5', '*U*', ',', '-', '--', '-LCB-', '-LRB-', '-RCB-', '-RRB-', '.', '...', '0', '0.1', '0.25', '1', '1,000', '1,500', '1.1', '1.2', '1.5', '1.65', '10', '10,000', '10-day', '10-year', '10.2', '100', '100,000', '11', '12', '12-point', '12.5', '120', '125', '13', '130', '14', '140', '15', '15,000', '150', '16', '16.7', '17', '17.3', '18', '180', '19', '1929', '195', '1970', '1972', '1976', '1977', '1979', '1980s', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1989-90', '1990', '1991', '1992', '1993', '1994', '1999', '1\\\\/2', '1\\\\/4', '1\\\\/8', '2', '2,000', '2,500', '2.1', '2.2', '2.50', '2.7', '20', '20,000', '200', '2009', '2019', '2029', '21', '22', '22\\\\/32', '23', '23.5', '24', '25', '25,000', '250', '27', '28', '29', '2\\\\/32', '3', '3.1', '3.18', '3.2', '3.3', '3.7', '3.8', '30', '30,000', '30-day', '30-year', '300', '31', '32', '33', '35', '3\\\\/4', '3\\\\/8', '4', '40', '400', '41', '42', '43', '44', '45', '45,000', '450', '46', '47', '47.6', '48', '49', '5', '5,000', '50', '50,000', '500', '500,000', '51', '52', '53', '55', '58', '59', '5\\\\/8', '6', '6.20', '6.40', '60', '61', '62', '63', '64', '65', '7', '7.272', '7.3', '7.458', '7.5', '7.88', '7.90', '70', '75', '750', '767', '7\\\\/8', '8', '8.5', '8.50', '80', '85', '9', '90', '900', '98', '99', ':', ';', '?', 'A', 'A.', 'ADRs', 'AT&T', 'About', 'According', 'Achievement', 'Ackerman', 'Acquisition', 'Act', 'Administration', 'After', 'Agency', 'Agriculture', 'Air', 'Airlines', 'Alan', 'Alfred', 'All', 'Allen', 'Also', 'Although', 'America', 'American', 'Americans', 'Among', 'An', 'Ana', 'Analysts', 'And', 'Angeles', 'Another', 'Anthony', 'Antonio', 'Apple', 'April', 'Arabia', 'Arthur', 'Article', 'Artist', 'As', 'Asia', 'Asian', 'Associates', 'Association', 'At', 'Atlanta', 'August', 'Australia', 'Australian', 'Average', 'B.', 'Backe', 'Bailey', 'Baker', 'Baldwin', 'Baltimore', 'Bancorp', 'Bank', 'Bankers', 'Banking', 'Banks', 'Barney', 'Barnum', 'Basham', 'Basic', 'Baum', 'Because', 'Beijing', 'Bell', 'Belt', 'Berliner', 'Bernstein', 'Besides', 'Beth', 'Big', 'Black', 'Board', 'Bob', 'Boca', 'Boesel', 'Bond', 'Bordeaux', 'Boston', 'Both', 'Bougainville', 'Bradley', 'Braidwood', 'Brazil', 'Brazilian', 'Bretz', 'Bridge', 'Britain', 'British', 'Brooklyn', 'Brothers', 'Buick', 'Burnham', 'Bush', 'Business', 'But', 'Buy', 'Buying', 'By', 'C.', 'CAT', 'CS', 'Cabernet', 'Calif', 'Calif.', 'California', 'Campbell', 'Canada', 'Canadian', 'Candela', 'Candlestick', 'Canepa', 'Cannell', 'Capital', 'Caribbean', 'Carla', 'Carlos', 'Carnival', 'Carolina', 'Center', 'Central', 'Chairman', 'Champagne', 'Chaplin', 'Chapter', 'Charles', 'Charlotte', 'Chase', 'Chateau', 'Chemical', 'Chevrolet', 'Chicago', 'China', 'Chinese', 'Christopher', 'Chrysler', 'Church', 'Citadel', 'City', 'Cleveland', 'Co', 'Co.', 'Coleman', 'College', 'Colo.', 'Columbia', 'Commerce', 'Commission', 'Committee', 'Commodity', 'Commonwealth', 'Communications', 'Community', 'Computer', 'Congress', 'Conn.', 'Connecticut', 'Constitution', 'Containers', 'Contel', 'Copperweld', 'Corp', 'Corp.', 'Corporate', 'Cosby', 'County', 'Court', 'Courter', 'Coxon', 'Crane', 'Cray', 'Cray-3', 'Credit', 'Creek', 'Cristal', 'Currently', 'Curry', 'Czechoslovakia', 'D.', 'D.C.', 'DD', 'DES', 'DSM', 'Daily', 'Dallara', 'Dallas', 'Darkhorse', 'David', 'Dealers', 'Dec.', 'December', 'Dell', 'Democrat', 'Democratic', 'Democrats', 'Department', 'Des', 'Despite', 'Detroit', 'Development', 'Diamond', 'Diaper', 'Dingell', 'Dinkins', 'District', 'Do', 'Dodge', 'Donald', 'Donaldson', 'Dorothy', 'Dorrance', 'Douglas', 'Dover', 'Dow', 'Dr.', 'Drexel', 'Drug', \"Dunkin'\", 'During', 'Dutch', 'E.', 'Each', 'Earlier', 'East', 'Eastern', 'Economic', 'Edelman', 'Edison', 'Education', 'Edward', 'Eggers', 'Egnuss', 'Einhorn', 'Electric', 'Embassy', 'Energy', 'England', 'English', 'Entertainment', 'Equitable', 'Equus', 'Esso', 'Estate', 'Europe', 'European', 'Even', 'Examiner', 'Exchange', 'Express', 'F', 'FEDERAL', 'FT-SE', 'FTC', 'Factory', 'Fairless', 'Fannie', 'Faulding', 'February', 'Fed', 'Federal', 'Fees', 'Finance', 'Financial', 'First', 'Fla.', 'Florio', 'Food', 'Foot', 'For', 'Force', 'Ford', 'Foreign', 'Foster', 'France', 'Francisco', 'Frank', 'Free', 'Freeport-McMoRan', 'French', 'Friends', 'From', 'Fujitsu', 'Fund', 'Futures', 'G.', 'GAF', 'GMAC', 'Ganes', 'Garbage', 'Gary', 'GenCorp', 'General', 'George', 'Georgia', 'Georgia-Pacific', 'Gerald', 'Germany', 'Giant', 'Giants', 'Giuliani', 'Glenn', 'Goldman', 'Government', 'Grace', 'Great', 'Greenville', 'Group', 'Growth', 'Guard', 'Guffey', 'Guinea', 'Gulf', 'H.', 'Hahn', 'Hammersmith', 'Hammond', 'Hampshire', 'Hampton', 'Harold', 'Harper', 'Hartford', 'He', 'Health', 'Hearst', 'Heavy', 'Heights', 'Henry', 'Her', 'Herald', 'Here', 'Heritage', 'High', 'Highway', 'Hill', 'Hills', 'His', 'Holding', 'Holdings', 'Hong', 'Hormats', 'House', 'Houston', 'However', 'Hudson', 'Human', 'Hungary', 'Hutton', 'Hymowitz', 'I', 'IBM', 'II', 'III', 'IRS', 'If', 'Ill.', 'Illinois', 'Illuminating', 'In', 'Inc', 'Inc.', 'Indeed', 'Index', 'Indiana', 'Indianapolis', 'Industrial', 'Industries', 'Industry', 'Information', 'Instead', 'Institute', 'Insurance', 'Intelogic', 'International', 'Investment', 'Investors', 'Iowa', 'Issues', 'It', 'Italian', 'Italy', 'Its', 'J.', 'Jack', 'Jaguar', 'Jamaica', 'James', 'January', 'Japan', 'Japanese', 'Jenrette', 'Jerry', 'Jersey', 'Jim', 'John', 'Johnson', 'Jones', 'Joseph', 'Journal', 'Jr.', 'Judge', 'July', 'June', 'Just', 'Justice', 'K.', 'Kaminski', 'Kansas', 'Katzenstein', 'Kent', 'Kidder', 'Kingdom', 'Klauser', 'Koito', 'Kong', 'Korea', 'L.', 'La', 'Labor', 'Lake', 'Lama', 'Lambert', 'Lane', 'Larry', 'Las', 'Last', 'Law', 'Leap', 'Learning', 'Lehman', 'Lewis', 'Life', 'Like', 'Little', 'Loan', 'London', 'London-based', 'Lorillard', 'Los', 'Louis', 'Ltd', 'Ltd.', 'Lufkin', 'Lynch', 'M.', 'M.D.', 'Machines', 'Macmillan\\\\/McGraw', 'Macmillan\\\\/McGraw-Hill', 'Madison', 'Mae', 'Magna', 'Malaysia', 'Management', 'Manhattan', 'Manufacturers', 'Many', 'March', 'Marie-Louise', 'Marine', 'Mark', 'Market', 'Markets', 'Markey', 'Marshall', 'Martin', 'Mary', 'Mason', 'Mass', 'Mass.', 'Massachusetts', 'Materials', 'Maxwell', 'May', 'McAlpine', 'McCormick', 'McGovern', 'McGraw-Hill', 'McMillin', 'Meanwhile', 'Media', 'Medical', 'Medicine', 'Merc', 'Mercantile', 'Meridian', 'Merrill', 'Messrs.', 'Mexico', 'Miami', 'Mich.', 'Michael', 'Michelin', 'Midwestern', 'Mining', 'Minister', 'Ministry', 'Minneapolis', 'Mitsubishi', 'Mitsui', 'Moines', 'Moleculon', 'Monday', 'Money', 'Montedison', 'Moody', 'Moore', 'More', 'Moreover', 'Morgan', 'Mortgage', 'Moscow', 'Most', 'Motor', 'Motors', 'Mr.', 'Mrs.', 'Ms.', 'Much', 'Municipal', 'Murray', 'N.C', 'N.J', 'N.J.', 'N.V.', 'N.Y.', 'NASD', 'NBI', 'NEC', 'NIH', 'NL', 'Nasdaq', 'National', 'Neither', 'Nekoosa', 'Net', 'Nev.', 'New', 'Newhouse', 'News', 'Newsweek', 'Nissan', 'Nixon', 'No', 'No.', 'None', 'Northeast', 'Northern', 'Not', 'Nov.', 'November', 'Now', 'OSHA', 'OTC', 'Oct.', 'October', 'Of', 'Officials', 'Ohio', 'Oil', 'Old', 'Olympic', 'On', 'Once', 'One', 'Only', 'Options', 'Or', 'Orange', 'Organization', 'Orleans', 'Oshkosh', 'Otero', 'Other', 'Oxnard', 'P.', 'PC', 'PCs', 'PLC', 'POP', 'PS', 'Pa.', 'Pacific', 'PaineWebber', 'Panama', 'Paper', 'Park', 'Parliament', 'Partly', 'Partners', 'Paul', 'Peabody', 'Pennsylvania', 'People', 'Peter', 'Petroleum', 'Petrus', 'Phelan', 'Philadelphia', 'Philippines', 'Pickens', 'Pictures', 'Pinocchio', 'Pittsburgh', 'Poland', 'Poor', 'Poore', 'Porter', 'Pratt', 'President', 'Previously', 'Price', 'Prime', 'Primerica', 'Profit', 'Program', 'Prudential-Bache', 'Public', 'Publishing', 'Put', 'R.', 'RMS', 'Radio', 'Rally', 'Randolph', 'Random', 'Rapanelli', 'Ratners', 'Reagan', 'Reed', 'Reliance', 'Rep.', 'Report', 'Representative', 'Republican', 'Research', 'Reserve', 'Resources', 'Retin-A', 'Reupke', 'Reuters', 'Revenue', 'Richard', 'Riese', 'Ringers', 'Robert', 'Rockefeller', 'Rockwell', 'Roederer', 'Romanee-Conti', 'Ross', 'Rowe', 'Rudolph', 'Russell', 'S&P', 'S.', 'SEC', 'Sachs', 'Sacramento', 'Safety', 'Sales', 'Salomon', 'Salon', 'Samnick', 'San', 'Santa', 'Saudi', 'Savin', 'Savings', 'Says', 'School', 'Scoring', 'Scott', 'Sea', 'Second', 'Secretary', 'Section', 'Securities', 'Sen.', 'Senate', 'Seoul', 'Separately', 'Sept.', 'September', 'Series', 'Service', 'Services', 'Several', 'Seymour', 'Shapiro', 'She', 'Shearson', 'Sheep', 'Sherwood', 'Sidewalk', 'Similarly', 'Simmons', 'Since', 'Singapore', 'Sir', 'Skills', 'Smith', 'So', 'Society', 'Some', 'Sonnett', 'Sony', 'Soup', 'Source', 'South', 'Southeast', 'Southern', 'Soviet', 'Soviets', 'Spain', 'Speculation', 'Spoon', 'Springs', 'St.', 'Stag', 'Stamford', 'Standard', 'Stanley', 'State', 'States', 'Stearn', 'Steinberg', 'Stephen', 'Sterling', 'Still', 'Stock', 'Stock-index', 'Stores', 'Stories', 'Street', 'Stronach', 'Such', 'Sullivan', 'Supreme', 'Swiss', 'Switzerland', 'System', 'Systems', 'T.', 'TV', 'Tache', 'Taipei', 'Taiwan', 'Take', 'Talcott', 'Taylor', 'Technology', 'Telegraph', 'Telephone', 'Telerate', 'Temple', 'Terms', 'Test', 'Texas', 'Thailand', 'That', 'The', 'Their', 'Then', 'There', 'These', 'They', 'This', 'Thomas', 'Those', 'Three', 'Thursday', 'Thus', 'Time', 'Times', 'Timex', 'To', 'Today', 'Tokyo', 'Tony', 'Toronto', 'Trade', 'Trading', 'Tramp', 'Transportation', 'Travelers', 'Treasury', 'Triton', 'Trotter', 'Trudeau', 'Trust', 'Tuesday', 'Two', 'Tyre', 'U.K.', 'U.S.', 'U.S.A.', 'UAL', 'US$', 'USIA', 'USX', 'Under', 'Union', 'United', 'University', 'Unlike', 'Until', 'Utilities', 'VOA', 'Valley', 'Vargas', 'Vegas', 'Veraldi', 'Viacom', 'Virginia', 'Voice', 'Volume', 'W.R.', 'Wa', 'Wakui', 'Wall', 'War', 'Ward', 'Washington', 'Watson', 'We', 'Wednesday', 'Weisfield', 'West', 'Western', 'What', 'Whelen', 'When', 'While', 'White', 'Whiting', 'Who', 'Why', 'Wild', 'Wilder', 'William', 'Williams', 'Wine', 'With', 'Without', 'Wolf', 'Wood', 'World', 'Wright', 'Yamamoto', 'Yang', 'Yeargin', 'Yesterday', 'Yet', 'York', 'York-based', 'You', 'Young', 'Younkers', 'Your', '`', '``', 'a', 'abandoned', 'ability', 'able', 'abortion', 'about', 'above', 'abuse', 'academic', 'accelerated', 'accept', 'accepted', 'access', 'accessories', 'accommodate', 'according', 'account', 'accounts', 'accused', 'achievement', 'acid', 'acid-rain', 'acne', 'acquire', 'acquired', 'acquisition', 'acquisitions', 'across', 'act', 'action', 'actions', 'active', 'activities', 'activity', 'acts', 'actual', 'ad', 'add', 'added', 'adding', 'addition', 'additional', 'adds', 'adjusted', 'administration', 'administrative', 'administrator', 'admits', 'admitting', 'adopted', 'ads', 'advance', 'advanced', 'advantage', 'advertise', 'advertisers', 'advertising', 'adviser', 'aerospace', 'affairs', 'affiliate', 'afraid', 'after', 'afternoon', 'again', 'against', 'age', 'agency', 'aggressive', 'ago', 'agree', 'agreed', 'agreement', 'ahead', 'aid', 'aide', 'aimed', 'air', 'airline', 'alcohol', 'all', 'alleged', 'allegedly', 'allow', 'allowed', 'almost', 'alone', 'along', 'already', 'also', 'alternative', 'alternatives', 'although', 'always', 'amendment', 'amid', 'among', 'amount', 'amounted', 'amounts', 'an', 'analyst', 'analysts', 'and', 'announced', 'announcement', 'announcer', 'annual', 'annually', 'another', 'answer', 'answers', 'anticipated', 'antitrust', 'any', 'anyone', 'anything', 'apparently', 'appeal', 'appeals', 'appear', 'appeared', 'appears', 'appliances', 'application', 'applications', 'applied', 'apply', 'appointment', 'appointments', 'approach', 'appropriations', 'approval', 'approve', 'approved', 'arbitrage', 'architecture', 'are', 'area', 'areas', 'argue', 'argued', 'arm', 'arms', 'around', 'arrangement', 'art', 'article', 'as', 'asbestos', 'asbestos-related', 'ask', 'asked', 'asking', 'asks', 'asset', 'assets', 'assistance', 'assistant', 'association', 'assume', 'assuming', 'assumption', 'at', 'attached', 'attack', 'attempt', 'attempting', 'attempts', 'attention', 'attorney', 'attorneys', 'attract', 'attributed', 'auction', 'auctioned', 'auctions', 'authorities', 'authority', 'authors', 'auto', 'automatic', 'automotive', 'available', 'average', 'avoid', 'awarded', 'aware', 'away', 'back', 'backed', 'backers', 'bad', 'badly', 'balance', 'ballot', 'ballpark', 'ballplayers', 'ban', 'band', 'bang', 'bank', 'banker', 'banking', 'bankruptcy', 'banks', 'banned', 'bar', 'barred', 'barrels', 'barrier', 'barriers', 'base', 'baseball', 'based', 'basic', 'basically', 'basis', 'basket', 'battle', 'be', 'bearing', 'beat', 'beaten', 'beautiful', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'began', 'begin', 'beginning', 'behind', 'being', 'belfry', 'believe', 'believed', 'believes', 'bell', 'bells', 'below', 'belts', 'benchmark', 'benefit', 'benefits', 'best', 'better', 'between', 'beyond', 'bid', 'bidders', 'bidding', 'bids', 'big', 'biggest', 'bilateral', 'bill', 'billion', 'billions', 'bills', 'black', 'blacks', 'blamed', 'block', 'blue', 'blue-chip', 'board', 'bond', 'bonds', 'bonus', 'book', 'booklets', 'books', 'booming', 'boost', 'boosted', 'boosts', 'borrowing', 'both', 'bottle', 'bottom', 'bought', 'box', 'branch', 'breaker', 'breaks', 'bridge', 'bridges', 'brief', 'bring', 'broad', 'broader', 'brokerage', 'brokers', 'brought', 'budget', 'build', 'building', 'buildings', 'built', 'bulk', 'bullets', 'buses', 'business', 'businesses', 'but', 'buy', 'buy-back', 'buy-out', 'buy-outs', 'buyer', 'buyers', 'buying', 'by', 'ca', 'call', 'called', 'calls', 'came', 'campaign', 'campaigns', 'can', 'cancer', 'candidates', 'capacity', 'capital', 'capped', 'capture', 'car', 'card', 'care', 'carried', 'carrier', 'carries', 'carry', 'cars', 'case', 'cases', 'cash', 'cataract', 'caught', 'cause', 'caused', 'causes', 'causing', 'ceiling', 'cent', 'center', 'centers', 'central', 'cents', 'century', 'certain', 'certainly', 'chain', 'chairman', 'challenge', 'chance', 'change', 'changed', 'changes', 'changing', 'charge', 'charged', 'charges', 'chassis', 'cheaper', 'cheating', 'check', 'checking', 'chemical', 'chemicals', 'chief', 'child', 'children', 'chip', 'chips', 'choose', 'choosing', 'chosen', 'church', 'churches', 'cigarette', 'cigarettes', 'circuit', 'circulation', 'circumstances', 'citations', 'cited', 'cites', 'cities', 'citing', 'citizen', 'city', 'claim', 'claims', 'class', 'classic', 'classroom', 'clause', 'clean-air', 'clear', 'clearly', 'client', 'clients', 'climbed', 'close', 'closed', 'closed-end', 'closely', 'closer', 'closing', 'club', 'collapse', 'collar', 'colleagues', 'collected', 'college', 'combined', 'come', 'comes', 'coming', 'comment', 'comments', 'commercial', 'commission', 'commissions', 'commitments', 'committed', 'committee', 'commodities', 'commodity', 'common', 'community', 'companies', 'company', 'comparable', 'compared', 'compensation', 'compete', 'competition', 'competitive', 'competitor', 'complained', 'complaint', 'complete', 'completed', 'completion', 'complex', 'compliance', 'complicated', 'composite', 'compound', 'compromise', 'computer', 'computers', 'concept', 'concern', 'concerns', 'concluded', 'conditions', 'conduct', 'conference', 'confidence', 'confidential', 'confirmed', 'conglomerate', 'congressional', 'connection', 'consecutive', 'consent', 'consented', 'consider', 'considerable', 'considerably', 'consideration', 'considered', 'considering', 'considers', 'consist', 'constantly', 'constitutional', 'construction', 'consultant', 'consulting', 'consumer', 'consumers', 'contacted', 'contain', 'contained', 'contemporary', 'contends', 'continually', 'continue', 'continued', 'continues', 'continuing', 'contract', 'contracts', 'contributed', 'control', 'controlled', 'controlling', 'controls', 'controversial', 'controversy', 'convenient', 'conventional', 'conversion', 'convertible', 'convicted', 'cooperation', 'cop-killer', 'copies', 'copy', 'copyright', 'corn', 'corporate', 'corporations', 'cost', 'cost-sharing', 'costly', 'costs', 'could', 'council', 'councils', 'countries', 'country', 'counts', 'couple', 'coupon', 'course', 'court', 'courts', 'cover', 'covered', 'covers', 'crash', 'create', 'created', 'creating', 'credit', 'creditors', 'crib', 'crime', 'criminal', 'crisis', 'critics', 'crocidolite', 'crop', 'crude', 'crunch', 'crystal', 'crystals', 'curb', 'currency', 'current', 'current-carrying', 'currently', 'curriculum', 'customer', 'customers', 'cut', 'cutbacks', 'cuts', 'cutting', 'daily', 'dam', 'damage', 'damaged', 'damages', 'data', 'date', 'day', 'days', 'de', 'deadline', 'deal', 'dealers', 'dealings', 'deals', 'death', 'deaths', 'debate', 'debentures', 'debt', 'debts', 'decade', 'decades', 'decide', 'decided', 'decision', 'decisions', 'declared', 'decline', 'declined', 'declines', 'declining', 'decrease', 'default', 'defendants', 'defense', 'defensive', 'deficit', 'degrees', 'delivered', 'delivery', 'demand', 'demise', 'democracy', 'denied', 'denies', 'denying', 'department', 'departure', 'deposit', 'depositary', 'deposits', 'deputy', 'described', 'deserve', 'design', 'designed', 'designer', 'designers', 'despite', 'details', 'determined', 'develop', 'developed', 'developing', 'development', 'devices', 'diaper', 'did', 'die', 'difference', 'differences', 'different', 'difficult', 'direct', 'direction', 'directly', 'director', 'directors', 'disappointed', 'disappointing', 'disappointment', 'disciplinary', 'disclosed', 'discount', 'discovered', 'discrepancies', 'discuss', 'discussed', 'discussing', 'discussions', 'disease', 'diseases', 'disgorge', 'disk', 'dismissed', 'disorders', 'disposal', 'dispute', 'disputes', 'dissemination', 'distribution', 'district', 'districts', 'disturbing', 'divided', 'dividend', 'dividends', 'division', 'do', 'documents', 'does', 'doing', 'dollar', 'dollar-denominated', 'dollars', 'domestic', 'dominated', 'done', 'door', 'doubled', 'doubt', 'down', 'downturn', 'dozen', 'dozens', 'draw', 'drawing', 'drawn', 'drink', 'drivers', 'driving', 'drop', 'dropped', 'drought', 'drug', 'drugs', 'dry', 'due', 'dumped', 'during', 'duties', 'duty', 'duty-free', 'each', 'eager', 'earlier', 'early', 'earned', 'earnings', 'ease', 'easier', 'easily', 'easy', 'economic', 'economics', 'economies', 'economist', 'economists', 'economy', 'edition', 'editor', 'editorial', 'editors', 'education', 'educators', 'effect', 'effective', 'effectively', 'effects', 'efficient', 'effort', 'efforts', 'eight', 'either', 'elaborate', 'elected', 'elections', 'electrical', 'electricity', 'electronic', 'electronics', 'elevators', 'eligible', 'eliminate', 'eliminated', 'eliminates', 'else', 'elsewhere', 'emissions', 'employed', 'employee', 'employees', 'employer', 'employment', 'employs', 'encourage', 'end', 'ended', 'ending', 'energy', 'enforcement', 'engineering', 'enough', 'entered', 'enters', 'entire', 'entirely', 'environment', 'environmental', 'equal', 'equally', 'equipment', 'equity', 'equivalent', 'era', 'especially', 'essentially', 'estate', 'estimated', 'estimates', 'ethical', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'evidence', 'evil', 'exact', 'examination', 'example', 'exceed', 'except', 'excess', 'excessive', 'exchange', 'exchanges', 'excision', 'exclusive', 'execute', 'executed', 'executing', 'execution', 'executive', 'executives', 'exercise', 'exercised', 'existing', 'expand', 'expanded', 'expanding', 'expansion', 'expect', 'expectations', 'expected', 'expecting', 'expects', 'expelled', 'expenses', 'expensive', 'experience', 'expert', 'experts', 'expire', 'explained', 'explains', 'export', 'exporter', 'exports', 'exposure', 'exposures', 'expressed', 'extend', 'extended', 'extent', 'extra', 'extraordinary', 'eye', 'face', 'facility', 'facing', 'fact', 'factor', 'factors', 'factory', 'fail', 'failed', 'failing', 'fairly', 'fall', 'fallen', 'falling', 'familiar', 'family', 'far', 'farm', 'farmers', 'fast', 'fast-food', 'faster', 'father', 'favor', 'favorable', 'favored', 'fear', 'feared', 'fears', 'features', 'federal', 'fee', 'feel', 'feeling', 'fees', 'fell', 'felt', 'female', 'fetal', 'fetal-tissue', 'few', 'fewer', 'fibers', 'field', 'fields', 'fifth', 'fifth-grade', 'fight', 'figure', 'figures', 'file', 'filed', 'filing', 'filings', 'film', 'films', 'filters', 'final', 'finally', 'finance', 'financed', 'financial', 'financially', 'financing', 'financings', 'find', 'finding', 'findings', 'fine', 'fined', 'fines', 'finished', 'fired', 'firm', 'firms', 'first', 'fiscal', 'five', 'fixed', 'fixed-rate', 'flag', 'flat', 'floating-rate', 'floor', 'flows', 'focus', 'focused', 'folks', 'followed', 'following', 'follows', 'food', 'for', 'force', 'forced', 'forces', 'forecast', 'forecasts', 'foreign', 'form', 'formed', 'former', 'forms', 'forward', 'found', 'founder', 'four', 'fourth', 'fourth-quarter', 'framers', 'francs', 'free', 'freedom', 'frequently', 'fresh', 'friend', 'friendly', 'friends', 'from', 'front', 'fuel', 'fueled', 'full', 'fully', 'fund', 'fundamental', 'funded', 'funding', 'funds', 'funny', 'furor', 'further', 'future', 'futures', 'gain', 'gained', 'gaining', 'gains', 'game', 'gave', 'general', 'generally', 'geography', 'get', 'gets', 'getting', 'giant', 'give', 'given', 'gives', 'giving', 'glass', 'global', 'globe', 'go', 'goal', 'goes', 'going', 'gold', 'gone', 'good', 'goods', 'got', 'gotten', 'government', 'governor', 'graders', 'grain', 'grant', 'great', 'greater', 'greatly', 'green', 'grew', 'ground', 'group', 'groups', 'growing', 'grown', 'grows', 'growth', 'guarantees', 'guards', 'guests', 'guild', 'guilders', 'guilty', 'gyrations', 'had', 'half', 'half-hour', 'halt', 'hand', 'handling', 'hands', 'happen', 'hard', 'harder', 'hardly', 'hardware', 'harsh', 'has', 'have', 'having', 'hazards', 'he', 'head', 'headquarters', 'heads', 'health', 'healthy', 'hear', 'heard', 'hearing', 'heart', 'heating', 'heavily', 'heavy', 'hefty', 'held', 'help', 'helped', 'helping', 'her', 'here', 'high', 'high-priced', 'high-tech', 'high-yield', 'higher', 'highest', 'highly', 'him', 'himself', 'hired', 'his', 'historic', 'historically', 'history', 'hit', 'hold', 'holders', 'holding', 'holds', 'home', 'homeless', 'homelessness', 'homes', 'homosexual', 'honor', 'hope', 'hopefully', 'hopes', 'host', 'hotel', 'hour', 'hours', 'house', 'household', 'houses', 'how', 'however', 'huge', 'human', 'hundred', 'hurt', 'husband', 'idea', 'ideas', 'identified', 'if', 'illegal', 'illness', 'image', 'immediately', 'imminent', 'impact', 'import', 'important', 'imported', 'imports', 'impose', 'impossible', 'improve', 'improved', 'improvement', 'improving', 'in', 'incentive', 'include', 'included', 'includes', 'including', 'income', 'incomplete', 'increase', 'increased', 'increases', 'increasing', 'increasingly', 'incurred', 'indeed', 'independent', 'index', 'index-arbitrage', 'indicate', 'indicated', 'indicates', 'indicator', 'indicators', 'individual', 'individuals', 'industrial', 'industry', 'inflation', 'influence', 'information', 'initial', 'initially', 'initiatives', 'insist', 'insisted', 'insists', 'instance', 'instead', 'institution', 'institutional', 'institutions', 'instruments', 'insurance', 'integrated', 'integration', 'intellectual-property', 'intended', 'intense', 'intention', 'interest', 'interest-rate', 'interested', 'interesting', 'interests', 'interim', 'internal', 'international', 'interpretation', 'interstate', 'interview', 'into', 'introduce', 'introduced', 'introduction', 'inventories', 'invested', 'investigating', 'investigation', 'investing', 'investment', 'investment-grade', 'investments', 'investor', 'investors', 'involve', 'involved', 'involving', 'is', 'issue', 'issued', 'issues', 'it', 'items', 'its', 'itself', 'job', 'jobs', 'join', 'joined', 'joining', 'joint', 'jointly', 'judge', 'judges', 'judicial', 'jumped', 'junk', 'junk-bond', 'just', 'justices', 'keep', 'keeping', 'kept', 'key', 'kids', 'killed', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows', 'labor', 'labor-management', 'lack', 'land', 'language', 'large', 'largely', 'larger', 'largest', 'last', 'late', 'later', 'latest', 'laughing', 'launch', 'launched', 'law', 'lawmakers', 'laws', 'lawsuit', 'lawyer', 'lawyers', 'lead', 'leader', 'leaders', 'leading', 'learned', 'least', 'leave', 'leaves', 'leaving', 'led', 'left', 'legal', 'legislation', 'legislative', 'lens', 'less', 'lesson', 'let', 'letter', 'letters', 'level', 'levels', 'leveraged', 'liability', 'liable', 'life', 'lift', 'light', 'like', 'likely', 'limit', 'limited', 'limits', 'line', 'lines', 'link', 'liquidity', 'list', 'listed', 'listing', 'litigation', 'little', 'live', 'lives', 'load', 'loan', 'loans', 'local', 'located', 'long', 'long-term', 'longer', 'longer-term', 'look', 'looking', 'looming', 'losing', 'loss', 'losses', 'lost', 'lot', 'low', 'low-ability', 'lower', 'lowered', 'lowest', 'loyalty', 'lucrative', 'lure', 'lying', 'machine', 'made', 'magazine', 'magnetic', 'magnitude', 'main', 'mainly', 'maintained', 'maintaining', 'maintenance', 'major', 'majority', 'make', 'maker', 'makers', 'makes', 'making', 'male', 'man', 'managed', 'management', 'manager', 'managers', 'managing', 'manufacturer', 'manufacturers', 'manufacturing', 'many', 'march', 'margin', 'margins', 'mark', 'market', 'market-share', 'marketed', 'marketing', 'marketplace', 'markets', 'marks', 'master', 'match', 'matched', 'material', 'materials', 'mathematics', 'matter', 'matters', 'mature', 'maturities', 'maturity', 'may', 'mayor', 'me', 'mean', 'means', 'meant', 'meanwhile', 'measure', 'measures', 'mechanical', 'mechanism', 'media', 'medical', 'medicine', 'medium-sized', 'meet', 'meeting', 'meetings', 'member', 'members', 'membership', 'memory', 'men', 'mental', 'mention', 'merchant', 'merchants', 'merely', 'merger', 'mergers', 'message', 'met', 'metric', 'middle', 'might', 'miles', 'military', 'milk', 'million', 'millions', 'minimum', 'minister', 'minivans', 'minority', 'minute', 'minutes', 'mixed', 'model', 'models', 'moderate', 'modest', 'modestly', 'moment', 'money', 'money-market', 'month', 'monthly', 'months', 'more', 'morning', 'mortgage', 'mortgage-backed', 'mortgages', 'most', 'mostly', 'mother', 'motion', 'motive', 'move', 'moved', 'moves', 'movie', 'moving', 'much', 'multi-crystal', 'municipalities', 'must', 'mutual', 'my', \"n't\", 'name', 'named', 'nation', 'national', 'nations', 'nature', 'near', 'nearly', 'necessary', 'need', 'needed', 'needs', 'negative', 'negotiate', 'negotiations', 'neither', 'net', 'network', 'never', 'new', 'newly', 'news', 'newspaper', 'newspapers', 'next', 'nice', 'night', 'nine', 'no', 'none', 'nonexecutive', 'nor', 'normal', 'normally', 'nose', 'not', 'notably', 'note', 'noted', 'notes', 'nothing', 'novel', 'now', 'nuclear', 'number', 'numbers', 'numerous', 'objective', 'obligation', 'observers', 'obtain', 'obtained', 'occur', 'occurred', 'odd', 'of', 'off', 'offenders', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'offices', 'official', 'officially', 'officials', 'offset', 'often', 'oil', 'old', 'older', 'on', 'once', 'one', 'one-hour', 'one-third', 'one-time', 'one-year', 'ones', 'only', 'open', 'opened', 'opening', 'operate', 'operates', 'operating', 'operations', 'operator', 'opinion', 'opportunity', 'oppose', 'opposed', 'optimism', 'option', 'options', 'or', 'order', 'ordered', 'orders', 'organization', 'organizations', 'original', 'originally', 'other', 'others', 'ought', 'our', 'out', 'outside', 'outstanding', 'over', 'over-the-counter', 'overall', 'overhead', 'overseas', 'owed', 'own', 'owned', 'owner', 'owners', 'ownership', 'owns', 'pace', 'package', 'packages', 'packaging', 'page', 'pages', 'paid', 'panel', 'panic', 'paper', 'par', 'parallels', 'parent', 'parents', 'park', 'parliament', 'part', 'participants', 'particular', 'particularly', 'parties', 'partly', 'partner', 'partners', 'partnership', 'parts', 'party', 'pass', 'passage', 'passed', 'passenger', 'past', 'patent', 'patents', 'pattern', 'patterns', 'pay', 'paying', 'payment', 'payments', 'payouts', 'penalties', 'pence', 'pending', 'pension', 'people', 'per', 'percentage', 'performance', 'performed', 'perhaps', 'period', 'peripheral', 'permission', 'permit', 'permitted', 'persistent', 'person', 'personal', 'persons', 'pharmaceutical', 'pharmaceuticals', 'pickers', 'picture', 'pill', 'place', 'placed', 'places', 'plaintiffs', 'plan', 'planned', 'plans', 'plant', 'plants', 'plastic', 'play', 'played', 'players', 'playing', 'pleased', 'plight', 'plunge', 'plunged', 'plus', 'point', 'pointed', 'points', 'police', 'policy', 'political', 'politicians', 'politics', 'poor', 'poorly', 'popular', 'population', 'portfolio', 'portfolios', 'portion', 'position', 'positions', 'positive', 'possibility', 'possible', 'possibly', 'post', 'posted', 'posts', 'potential', 'pound', 'pounds', 'power', 'powers', 'practice', 'practices', 'precedent', 'precisely', 'preferred', 'preliminary', 'premiere', 'premium', 'premiums', 'preparation', 'prepared', 'prerogatives', 'presence', 'presidency', 'president', 'press', 'pressure', 'pressured', 'pressures', 'prestigious', 'pretax', 'pretty', 'prevent', 'preventing', 'previous', 'previously', 'price', 'priced', 'prices', 'primarily', 'primary', 'principal', 'print', 'printed', 'prints', 'prior', 'priority', 'prison', 'private', 'privately', 'probably', 'problem', 'problems', 'procedures', 'proceedings', 'process', 'produce', 'produced', 'producer', 'producers', 'producing', 'product', 'production', 'products', 'professionals', 'professor', 'profit', 'profit-taking', 'profitability', 'profitable', 'profits', 'program', 'program-trading', 'programming', 'programs', 'progress', 'prohibits', 'project', 'projects', 'promise', 'promote', 'promotion', 'prompted', 'promptly', 'propaganda', 'property', 'proposal', 'proposed', 'proposing', 'prosecutors', 'prospects', 'protect', 'protecting', 'protection', 'prove', 'provide', 'provided', 'provides', 'providing', 'proving', 'provision', 'provisions', 'psychiatric', 'psychology', 'public', 'publicly', 'publisher', 'publishes', 'publishing', 'purchase', 'purchased', 'purchases', 'purchasing', 'purpose', 'pursuant', 'pursue', 'pursued', 'push', 'put', 'quality', 'quantity', 'quarter', 'quarterly', 'question', 'questionable', 'questions', 'quick', 'quickly', 'quiet', 'quite', 'quota', 'quoted', 'race', 'radio', 'railcars', 'railing', 'railings', 'rain', 'raise', 'raised', 'raises', 'raising', 'ran', 'range', 'ranged', 'rape', 'rapidly', 'rate', 'rates', 'rather', 'rating', 'ratings', 'reach', 'reached', 'reaction', 'read', 'reading', 'real', 'real-estate', 'reality', 'realize', 'really', 'rear-seat', 'reason', 'reasons', 'rebound', 'receipts', 'receive', 'received', 'receiving', 'recent', 'recently', 'recession', 'recognition', 'recognize', 'recommendations', 'record', 'records', 'redeemed', 'reduce', 'reduced', 'reducing', 'reduction', 'reductions', 'referred', 'reflect', 'reflected', 'reflecting', 'reflection', 'reflects', 'reform', 'reforms', 'refund', 'refunding', 'refused', 'regarding', 'region', 'regional', 'registered', 'registration', 'regular', 'regulation', 'regulators', 'regulatory', 'reinstatement', 'rejected', 'related', 'relations', 'relative', 'relatively', 'released', 'remain', 'remainder', 'remained', 'remaining', 'remains', 'remove', 'removed', 'renewed', 'reopen', 'reorganization', 'repair', 'replace', 'replaced', 'replacement', 'report', 'reported', 'reporters', 'reporting', 'reports', 'represent', 'represented', 'representing', 'represents', 'reputation', 'request', 'requested', 'require', 'required', 'requirement', 'requirements', 'requires', 'reruns', 'research', 'researcher', 'researchers', 'reserves', 'residential', 'resignation', 'resigned', 'resistance', 'resolution', 'resolve', 'resources', 'respected', 'respond', 'responded', 'response', 'responses', 'responsibilities', 'responsibility', 'responsible', 'rest', 'restaurants', 'restore', 'restrictions', 'restructured', 'restructuring', 'result', 'resulting', 'results', 'retail', 'retailing', 'retain', 'retired', 'return', 'returned', 'returns', 'revenue', 'review', 'revive', 'rich', 'rider', 'right', 'rights', 'ring', 'ringer', 'ringers', 'ringing', 'rise', 'rising', 'risk', 'risks', 'rival', 'road', 'role', 'roll', 'roof-crush', 'ropes', 'rose', 'roughly', 'round', 'rule', 'ruled', 'rules', 'ruling', 'rumored', 'rumors', 'run', 'running', 'runs', 'safety', 'said', 'salary', 'sale', 'sales', 'same', 'samples', 'sanctions', 'savings', 'saw', 'say', 'saying', 'says', 'scenes', 'scheduled', 'school', 'schools', 'scientific', 'scientists', 'scores', 'screen', 'scrutiny', 'season', 'seat', 'seats', 'second', 'second-largest', 'secondary', 'secretary', 'section', 'sector', 'securities', 'security', 'see', 'seeing', 'seek', 'seeking', 'seeks', 'seem', 'seemed', 'seems', 'seen', 'segment', 'segments', 'sell', 'selling', 'sells', 'semiannual', 'send', 'senior', 'sense', 'sent', 'sentiment', 'separate', 'separation', 'series', 'serious', 'seriously', 'serve', 'service', 'services', 'session', 'set', 'sets', 'settle', 'settled', 'settlement', 'seven', 'seven-day', 'several', 'shall', 'shame', 'shape', 'share', 'shareholder', 'shareholders', 'shares', 'sharp', 'sharply', 'she', 'sheets', 'ship', 'shipments', 'ships', 'shops', 'short', 'short-term', 'shortly', 'shot', 'should', 'shoulder', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'sign', 'signal', 'signals', 'signed', 'significant', 'significantly', 'signs', 'similar', 'simple', 'simply', 'since', 'single', 'situation', 'six', 'six-month', 'sixth', 'sizable', 'size', 'skeptical', 'skin', 'sleep', 'slide', 'slightly', 'slipped', 'slow', 'slowdown', 'slower', 'slowing', 'slowly', 'sluggish', 'small', 'smaller', 'smoking', 'snapped', 'so', 'so-called', 'soared', 'social', 'society', 'software', 'sold', 'solid', 'some', 'someone', 'something', 'sometimes', 'soon', 'sophisticated', 'sort', 'sought', 'sound', 'sounds', 'source', 'sources', 'space', 'sparked', 'special', 'specialist', 'specialists', 'specialty', 'specified', 'specify', 'speculated', 'speculation', 'speculative', 'speculators', 'speech', 'speed', 'spend', 'spending', 'spent', 'spinoff', 'spirit', 'split', 'spokesman', 'spokeswoman', 'sports', 'spread', 'spring', 'squeeze', 'stable', 'stadium', 'staff', 'stages', 'stake', 'stand', 'standard', 'standardized', 'standards', 'standing', 'stands', 'start', 'started', 'starting', 'starts', 'state', 'statement', 'states', 'station', 'stations', 'statistics', 'status', 'statute', 'stay', 'steady', 'stemming', 'step', 'stepped', 'stepping', 'still', 'stock', 'stock-index', 'stock-market', 'stockbrokers', 'stocks', 'stone', 'stood', 'stop', 'stopped', 'stores', 'story', 'straight', 'strategies', 'strategy', 'street', 'streets', 'strength', 'strict', 'strike', 'strong', 'stronger', 'structures', 'student', 'students', 'studied', 'study', 'stupid', 'subcommittee', 'subject', 'subsidiaries', 'subsidiary', 'substance', 'substantial', 'substantially', 'succeed', 'success', 'successful', 'successor', 'such', 'suffer', 'sugar', 'suggest', 'suggested', 'suggests', 'suit', 'summer', 'supercomputer', 'superconductor', 'superconductors', 'suppliers', 'supplies', 'supply', 'support', 'supported', 'sure', 'surge', 'surged', 'surprise', 'surprising', 'surprisingly', 'surrendered', 'survey', 'survival', 'suspect', 'suspend', 'suspended', 'suspension', 'swap', 'sweeping', 'swiftly', 'swing', 'swings', 'switch', 'system', 'systems', 'take', 'taken', 'takeover', 'takes', 'taking', 'talk', 'talking', 'talks', 'target', 'targeting', 'targets', 'taught', 'tax', 'taxes', 'taxpayers', 'teach', 'teacher', 'teachers', 'teaching', 'team', 'teams', 'technical', 'techniques', 'technology', 'telecommunications', 'telephone', 'television', 'telling', 'tells', 'temporarily', 'tend', 'tender', 'tendered', 'tentatively', 'term', 'terms', 'test', 'tested', 'testing', 'tests', 'text', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'theory', 'there', 'these', 'they', 'thin', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'third-quarter', 'this', 'those', 'though', 'thought', 'thousands', 'threatened', 'three', 'thrift', 'through', 'throughout', 'thus', 'ticket', 'tied', 'ties', 'tight', 'time', 'times', 'tire', 'to', 'today', 'together', 'told', 'tomorrow', 'tons', 'too', 'took', 'top', 'total', 'totaled', 'touch', 'toward', 'tower', 'town', 'trade', 'traded', 'trader', 'traders', 'trades', 'trading', 'traditional', 'trailed', 'training', 'transaction', 'transactions', 'transplants', 'travel', 'treat', 'treatment', 'tree', 'trend', 'trial', 'tried', 'triggered', 'trillion', 'trouble', 'troubled', 'troubles', 'trucks', 'true', 'trust', 'try', 'trying', 'tumbled', 'turn', 'turned', 'turning', 'turns', 'two', 'two-year', 'type', 'types', 'typically', 'unable', 'uncertainty', 'unchanged', 'unconstitutional', 'under', 'underlying', 'understand', 'understanding', 'underwriter', 'underwriters', 'unfair', 'union', 'unit', 'units', 'university', 'unless', 'unlike', 'until', 'unusual', 'up', 'upheld', 'upon', 'upscale', 'uptick', 'urged', 'us', 'use', 'used', 'uses', 'using', 'usual', 'usually', 'utilities', 'utility', 'v.', 'vacation', 'value', 'valued', 'values', 'van', 'vans', 'variety', 'various', 'varying', 'vehicle', 'vehicles', 'venture', 'version', 'very', 'veto', 'via', 'vicar', 'vice', 'victim', 'victims', 'view', 'viewed', 'viewpoint', 'views', 'violate', 'violations', 'virtually', 'visit', 'voice', 'volatile', 'volatility', 'volume', 'vote', 'voted', 'voters', 'wage', 'wait', 'wake', 'walk', 'want', 'wanted', 'wants', 'war', 'warning', 'warrants', 'was', 'waste', 'watch', 'watchers', 'watches', 'way', 'ways', 'we', 'weak', 'weaken', 'week', 'weekly', 'weeks', 'well', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'white', 'who', 'whom', 'whose', 'why', 'wide', 'widely', 'widespread', 'widget', 'will', 'willing', 'win', 'wine', 'wines', 'winning', 'wish', 'with', 'withdraw', 'withdrawal', 'withdrawn', 'within', 'without', 'wo', 'woman', 'women', 'won', 'word', 'words', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'world', 'world-wide', 'worried', 'worries', 'worry', 'worse', 'worth', 'would', 'writer', 'written', 'wrong', 'wrongdoing', 'wrote', 'year', 'year-earlier', 'years', 'yen', 'yesterday', 'yet', 'yield', 'yielding', 'yields', 'you', 'young', 'your', 'youth']\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "for key, value in dict(tokens_dict).items():\n",
    "    if value >= 3:\n",
    "        L.append(key)\n",
    "\n",
    "print(L)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εδώ ελέγχω πόσα tokens αφαιρέθηκαν συνολικά από το vocabulary γιατί είχαν εμφανιστεί λιγότερες από 3 φορές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 11683 total tokens 8010 tokens were removed from the training dictionary becuase they appeared less than 3 times in our corpus.\n"
     ]
    }
   ],
   "source": [
    "print('Out of', len(tokens_dict), 'total tokens', len(tokens_dict)-len(L), 'tokens were removed from the training dictionary becuase they appeared less than 3 times in our corpus.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια, σύμφωνα με τις οδηγίες τις εκφώνησης, αντικαθιστούμε όλα τα tokens που τελικά δεν μπήκαν στο τελικό μας vocabulary με το token **\\<UNK>**. Αυτό θα γίνει τόσο στο training όσο και στο test set. Για τη διευκόλυνση της διαδικασίας αυτής χρησιμοποιούμε ένα function που αντικαθιστά όλα τα tokens που πρέπει να αντικατασταθούν την οποία καλούμε τόσο για το training όσο και για το test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceLowFrequencyTokensWithUNK(set, vocabulary):\n",
    "    new_set = []\n",
    "    for sentence in set:\n",
    "        new_sentence = []\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                new_token = '<UNK>'\n",
    "            else:\n",
    "                new_token = token\n",
    "            new_sentence.append(new_token)\n",
    "        new_set.append(new_sentence)\n",
    "    return new_set\n",
    "\n",
    "training_set_with_UNK = replaceLowFrequencyTokensWithUNK(training_set_sentences, L)\n",
    "test_set_with_UNK = replaceLowFrequencyTokensWithUNK(test_set_sentences, L)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ακολούθως προσθέτουμε τα tokens **\\<BOS>** και **\\<EOS>** στην αρχή και το τέλος της κάθε πρότασης, αντίστοιχα. Και πάλι αυτό θα πρέπει να γίνει και στο training και στο test set και γίνεται με τη βοήθεια της function που ακολουθεί. Στη function χρησιμοποιείται η pad_both_ends από τη βιβλιοθήκη nltk.lm.preprocessing, η οποία μας επιτρέπει να προσθέσουμε ένα token αρχής και ένα token τέλους κάθε πρότασης. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBOSandEOS(set):\n",
    "    padded_set = []\n",
    "    for sentence in set:\n",
    "        padded_sentence = list(pad_both_ends(sentence, n=2, pad_left=True, left_pad_symbol=\"<BOS>\", pad_right=True, right_pad_symbol=\"<EOS>\"))\n",
    "        padded_set.append(padded_sentence)\n",
    "    return padded_set\n",
    "\n",
    "final_training_set = addBOSandEOS(training_set_with_UNK)\n",
    "final_test_set = addBOSandEOS(test_set_with_UNK)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training και Smoothing\n",
    "Το επόμενο κομμάτι είναι το training και το smoothing των ngram μοντέλων που θα δημιουργήσουμε. Για το σκοπό αυτό δημιουργώ μία συνάρτηση η οποία θα χρησιμοποιηθεί για τις διάφορες τιμές του k που ζητάει η εκφώνηση (0.01 και 1). Η συνάρτηση αυτή παίρνει ως είσοδο το training set, την τιμή του k (για το smoothing) και το λεξιλόγιο L και επιστρέφει τις πιθανότητες των bigrams και των unigrams μετά το smoothing. Στην πραγματικότητα το μοντέλο είναι οι πιθανότητες των bigrams στην έξοδο της συνάρτησης, ωστόσο χρειαζόμαστε και τις τιμές των unigrams για τον υπολογισμό του perplexity στο test set. Είναι σημαντικό να σημειωθεί επίσης ότι για το μέγεθος του vocabulary (V), πρέπει να προσθέσουμε +1 καθώς στις προτάσεις του training set αρκετές λέξεις έχουν αντικαστασταθεί από το ειδικό token \\<UNK>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBigramModelWithKSmoothing(set, k, vocabulary):\n",
    "    V = len(vocabulary)+1\n",
    "\n",
    "    raw_unigrams = []\n",
    "    for sentence in set:\n",
    "        for token in sentence:\n",
    "            raw_unigrams.append(token)\n",
    "\n",
    "    unigrams, counts = np.unique(raw_unigrams, return_counts=True) \n",
    "    unigrams_dict = dict(zip(unigrams, counts))\n",
    "\n",
    "    train_unigrams= {}\n",
    "    for key,value in unigrams_dict.items():\n",
    "        train_unigrams[key] = k/float(unigrams_dict[key]+k*V)\n",
    "\n",
    "    bigrams_dict = {}\n",
    "    for sentence in set:\n",
    "        for i in range(len(sentence)):\n",
    "            if i+1 == len(sentence): \n",
    "                break\n",
    "            elif (sentence[i], sentence[i+1]) in bigrams_dict:\n",
    "                bigrams_dict[(sentence[i], sentence[i+1])] += 1\n",
    "            else:\n",
    "                bigrams_dict[(sentence[i], sentence[i+1])] = 1\n",
    "\n",
    "    train_bigrams= {}\n",
    "    for key,value in bigrams_dict.items():\n",
    "        train_bigrams[key] = (value+k)/float(unigrams_dict[key[0]]+k*V)\n",
    "    \n",
    "    return train_bigrams, train_unigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Καλώντας τη συνάρτηση που δημιουργήσαμε παραπάνω γίνεται το training και το smoothing των bigram models για τιμές του k 0.01 και 1.0, αντίστοιχα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_k1_model, unigrams_k1 = trainBigramModelWithKSmoothing(final_training_set, 1.0, L)\n",
    "bigram_k001_model, unigrams_k001 = trainBigramModelWithKSmoothing(final_training_set, 0.01, L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια δημιουργούμε μία νέα συνάρτηση για την εκπαίδευση των μοντέλων τριγραμμάτων. Και πάλι ως είσοδος δίνεται το training set, η τιμή του k και το vocabulary και αυτή τη φορά η συνάρτηση επιστρέφει το μοντέλο τριγραμμάτων (πιθανότητες) αλλά και τις πιθανότητες των διγραμμάτων οι οποίες θα χρησιμοποιηθούν αργότερα για τον υπολογισμό του perplexity. Και εδώ προστίθεται +1 στο μέγεθος του λεξιλογίου για το ειδικό token \\<UNK>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTrigramModelWithKSmoothing(set, k, vocabulary):\n",
    "    V = len(vocabulary)+1\n",
    "\n",
    "    bigrams_dict = {}\n",
    "    for sentence in set:\n",
    "        for i in range(len(sentence)):\n",
    "            if i+1 == len(sentence): \n",
    "                break\n",
    "            elif (sentence[i], sentence[i+1]) in bigrams_dict:\n",
    "                bigrams_dict[(sentence[i], sentence[i+1])] += 1\n",
    "            else:\n",
    "                bigrams_dict[(sentence[i], sentence[i+1])] = 1\n",
    "    \n",
    "    train_bigrams= {}\n",
    "    for key,value in bigrams_dict.items():\n",
    "        train_bigrams[key] = k/float(bigrams_dict[key]+k*V)\n",
    "\n",
    "    trigrams_dict = {}\n",
    "    for sentence in set:\n",
    "        for i in range(len(sentence)):\n",
    "            if i+2 == len(sentence): \n",
    "                break\n",
    "            elif (sentence[i], sentence[i+1], sentence[i+2]) in trigrams_dict:\n",
    "                trigrams_dict[(sentence[i], sentence[i+1], sentence[i+2])] += 1\n",
    "            else:\n",
    "                trigrams_dict[(sentence[i], sentence[i+1], sentence[i+2])] = 1   \n",
    "    \n",
    "    train_trigrams= {}\n",
    "    for key,value in trigrams_dict.items():\n",
    "        train_trigrams[key] = (value+k)/float(bigrams_dict[(key[0],key[1])]+k*V)\n",
    "    \n",
    "    return train_trigrams, train_bigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Καλώντας τη συνάρτηση που δημιουργήσαμε παραπάνω παίρνουμε τα μοντέλα τριγραμμάτων για τις 2 τιμές του k που ζητούνται από την εκφώνηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_k1_model, bigrams_k1 = trainTrigramModelWithKSmoothing(final_training_set, 1.0, L)\n",
    "trigram_k001_model, bigrams_k001 = trainTrigramModelWithKSmoothing(final_training_set, 0.01, L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Υπολογισμός του Perplexity και Evaluation των Μοντέλων\n",
    "Στο επόμενο κομμάτι εκτιμάται η αποτελεσματικότητα κάθε μοντέλου στο test set που ξεχωρίσαμε προηγουμένως, δηλαδή στα τελευταία 29 κείμενα που κρατήσαμε από τα 199. Για τον υπολογισμό του perplexity χρησιμοποιείται ο τύπος που δίνεται στην εκφώνηση της άσκησης."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά δημιουργούμε μία συνάρτηση για τον υπολογισμό του perplexity στα μοντέλα διγραμμάτων. Η συνάρτηση λαμβάνει ως είσοδο το test set, την τιμή του k, τις πιθανότητες των bigrams και τις πιθανότητες των unigrams από το μοντέλο μας, καθώς και το λεξιλόγιο και επιστρέφει το perplexity. Και εδώ, όπως και στις προηγούμενες δύο συναρτήσεις προστίθεται +1 στο μέγεθος του λεξιλογίου καθώς υπάρχει και το token \\<UNK> το οποίο δεν είχαμε υπολογίσει δημιουργώντας το λεξιλόγιο αρχικά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bigram_model(k, bigram_probs, unigram_probs, test_set, vocabulary):\n",
    "    log_prob_sum = 0\n",
    "    N = 0\n",
    "    V = len(vocabulary) + 1\n",
    "    for sentence in test_set:\n",
    "        for i in range(1, len(sentence)):\n",
    "            ngram = tuple(sentence[i-1:i+1])\n",
    "            n_1_gram = ngram[:-1]\n",
    "            if ngram in bigram_probs:\n",
    "                prob = bigram_probs[ngram]\n",
    "            elif n_1_gram in unigram_probs:\n",
    "                prob = unigram_probs[n_1_gram]\n",
    "            else:\n",
    "                prob = k / (k * V)\n",
    "            log_prob_sum += np.log(prob)\n",
    "            N += 1\n",
    "    perplexity = round(np.exp(-log_prob_sum / N), 3)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια δημιουργείται και η συνάρτηση για το perplexity των μοντέλων για τα τριγράμματα. Η συνάρτηση αυτή παίρνει ως είσοδο τις πιθανότητες των trigrams και τις πιθανότητες των bigrams από το μοντέλο μας, την τιμή του k, το test set και το λεξιλόγιο και επιστρέφει την τιμή του perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trigram_model(k, trigram_probs, bigram_probs, test_set, vocabulary):\n",
    "    log_prob_sum = 0\n",
    "    N = 0\n",
    "    V = len(vocabulary) + 1\n",
    "    for sentence in test_set:\n",
    "        for i in range(2, len(sentence)):\n",
    "            ngram = tuple(sentence[i-2:i+1])\n",
    "            n_1_gram = ngram[:-1]\n",
    "            if ngram in trigram_probs:\n",
    "                prob = trigram_probs[ngram]\n",
    "            elif n_1_gram in bigram_probs:\n",
    "                prob = bigram_probs[n_1_gram]\n",
    "            else:\n",
    "                prob = k / (k * V)\n",
    "            log_prob_sum += np.log(prob)\n",
    "            N += 1\n",
    "    perplexity = round(np.exp(-log_prob_sum / N), 3)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας τη συνάρτηση για τα bigrams παίρνουμε τις τιμές του perplexity για τις 2 τιμές του k που εξετάστηκαν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the bigram model with k=0.01 on the test data is: 98.011\n",
      "The perplexity of the bigram model with k=1.0 on the test data is: 370.851\n"
     ]
    }
   ],
   "source": [
    "print('The perplexity of the bigram model with k=0.01 on the test data is:', evaluate_bigram_model(0.01, bigram_k001_model, unigrams_k001, final_test_set, L))\n",
    "print('The perplexity of the bigram model with k=1.0 on the test data is:', evaluate_bigram_model(1, bigram_k1_model, unigrams_k1, final_test_set, L))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αντίστοιχα παίρνουμε και τις τιμές για τα μοντέλα των trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the trigram model with k=0.01 on the test data is: 463.891\n",
      "The perplexity of the trigram model with k=1.0 on the test data is: 1505.009\n"
     ]
    }
   ],
   "source": [
    "print('The perplexity of the trigram model with k=0.01 on the test data is:', evaluate_trigram_model(0.01, trigram_k001_model, bigrams_k001, final_test_set, L))\n",
    "print('The perplexity of the trigram model with k=1.0 on the test data is:', evaluate_trigram_model(1, trigram_k1_model, bigrams_k1, final_test_set, L))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Μετατροπή σε Πεζά Γράμματα και εκ Νέου Εκπαίδευση των Μοντέλων"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο επόμενο κομμάτι της άσκησης θα πρέπει να μετατρέψουμε όλα τα κείμενα σε πεζά γράμματα και να εξετάσουμε τι συμβαίνει με το perplexity (αν αυξάνεται η μειώνεται). Αρχικά διαβάζουμε τις προτάσεις και μετατρέπουμε όλα τα tokens ένα προς 1 σε lower με χρήση του σχετικού function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = treebank.fileids()\n",
    "\n",
    "training_set_sentences_lower = []\n",
    "for file in files[:170]:\n",
    "    for sentence in treebank.sents(file):\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            new_word = word.lower()\n",
    "            new_sentence.append(new_word)\n",
    "        training_set_sentences_lower.append(new_sentence)\n",
    "\n",
    "test_set_sentences_lower = []\n",
    "for file in files[170:]:\n",
    "    for sentence in treebank.sents(file):\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            new_word = word.lower()\n",
    "            new_sentence.append(new_word)\n",
    "        test_set_sentences_lower.append(new_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ακολούθως δημιουργούμε το λεξιλόγιο και παρατηρούμε ότι αυτή τη φορά είναι μικρότερο σε μέγεθος, καθώς αρκετά tokens της μορφής \"Token\" και \"token\" τώρα θα λογίζονται ως ένα token και επομένως ο αριθμός των διαφορετικών tokens θα μειωθεί."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 10730 total tokens 7258 tokens were removed from the training dictionary becuase they appeared less than 3 times in our corpus.\n"
     ]
    }
   ],
   "source": [
    "training_tokens_lower = []\n",
    "for sentence in training_set_sentences_lower:\n",
    "    training_tokens_lower.extend(sentence)\n",
    "\n",
    "tokens, counts = np.unique(training_tokens_lower, return_counts=True) \n",
    "tokens_dict_lower = dict(zip(tokens, counts))\n",
    "\n",
    "L_lower = []\n",
    "for key, value in dict(tokens_dict_lower).items():\n",
    "    if value >= 3:\n",
    "        L_lower.append(key)\n",
    "\n",
    "print('Out of', len(tokens_dict_lower), 'total tokens', len(tokens_dict_lower)-len(L_lower), 'tokens were removed from the training dictionary becuase they appeared less than 3 times in our corpus.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια αντικαθιστούμε και πάλι τα tokens που εμφανίζονται λιγότερο από 3 φορές με το ειδικό token \\<UNK>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_with_UNK_lower = replaceLowFrequencyTokensWithUNK(training_set_sentences_lower, L_lower)\n",
    "test_set_with_UNK_lower = replaceLowFrequencyTokensWithUNK(test_set_sentences_lower, L_lower)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και ακολούθως προσθέτουμε τα \\<BOS> και \\<EOS> στην αρχή και το τέλος κάθε πρότασης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_set_lower = addBOSandEOS(training_set_with_UNK_lower)\n",
    "final_test_set_lower = addBOSandEOS(test_set_with_UNK_lower)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Γίνεται η εκπαίδευση και το smoothing για τα μοντέλα διγραμμάτων με τιμές του k 0.01 και 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_k1_model_lower, unigrams_k1_lower = trainBigramModelWithKSmoothing(final_training_set_lower, 1.0, L_lower)\n",
    "bigram_k001_model_lower, unigrams_k001_lower = trainBigramModelWithKSmoothing(final_training_set_lower, 0.01, L_lower)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και για τα μοντέλα τριγραμμάτων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_k1_model_lower, bigrams_k1_lower = trainTrigramModelWithKSmoothing(final_training_set_lower, 1.0, L_lower)\n",
    "trigram_k001_model_lower, bigrams_k001_lower = trainTrigramModelWithKSmoothing(final_training_set_lower, 0.01, L_lower)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και υπολογίζουμε το perplexity στα νέα test data για τα διγράμματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the bigram model with k=0.01 on the test data is: 100.63\n",
      "The perplexity of the bigram model with k=1.0 on the test data is: 370.55\n"
     ]
    }
   ],
   "source": [
    "print('The perplexity of the bigram model with k=0.01 on the test data is:', evaluate_bigram_model(0.01, bigram_k001_model_lower, unigrams_k001_lower, final_test_set_lower, L_lower))\n",
    "print('The perplexity of the bigram model with k=1.0 on the test data is:', evaluate_bigram_model(1, bigram_k1_model_lower, unigrams_k1_lower, final_test_set_lower, L_lower))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και για τα τριγράμματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the trigram model with k=0.01 on the test data is: 461.861\n",
      "The perplexity of the trigram model with k=1.0 on the test data is: 1470.946\n"
     ]
    }
   ],
   "source": [
    "print('The perplexity of the trigram model with k=0.01 on the test data is:', evaluate_trigram_model(0.01, trigram_k001_model_lower, bigrams_k001_lower, final_test_set_lower, L_lower))\n",
    "print('The perplexity of the trigram model with k=1.0 on the test data is:', evaluate_trigram_model(1, trigram_k1_model_lower, bigrams_k1_lower, final_test_set_lower, L_lower))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Συμπεράσματα\n",
    "Από την μετατροπή των κειμένων σε πεζά γράμματα παρατηρούμε ότι το perplexity και στα δύο μοντέλα διγραμμάτων αυξήθηκε, ενώ το perplexity στα μοντέλα τριγραμμάτων μειώθηκε. Λογικά, θα περιμέναμε ότι το perplexity του μοντέλου θα μειωνόταν καθώς πλέον διγράμματα όπως για παράδειγμα το ('Hello', 'world') και το ('hello', 'world') θα λογιζόνται πλέον ως το ίδιο δίγραμμα, γεγονός που θα επιτρέπει στο μοντέλο μας να καταλάβει ευκολότερα ότι μετά τη λέξη 'hello' θα πρέπει να μπει η λέξη 'world'.\n",
    "\n",
    "\n",
    "Στα μοντέλα τριγραμμάτων που δημιουργήσαμε, βλέπουμε ότι το perplexity μειώθηκε όταν όλες οι λέξεις έγιναν πεζές, γεγονός που δείχνει ότι τα μοντέλα αυτά πράγματι βελτιώθηκαν. Στα μοντέλα διγραμμάτων, ωστόσο, το perplexity αυξήθηκε επομένως άλλοι παράγοντες επηρέασαν την απόδοση των μοντέλων αυτών. Για παράδειγμα, ορισμένες λέξεις μπορεί να χάνουν το νόημά τους όταν αλλάζει το capitalization, γεγονός που ενδεχομένως δημιουργεί σύγχυση (όπως ας πούμε 'Turkey' η χώρα, όταν δεν είναι κεφαλαίο το Τ μπορεί να εκλαμβάνεται ως 'turkey' γαλοπούλα). Αντίστοιχα προβλήματα μπορεί να προκύψουν και με ονόματα τα οποία εκλαμβάνονται ως λέξεις. Για παράδειγμα το όνομα 'Ray' όταν δεν είναι κεφαλαίο το R μπορεί να δημιουργεί σύγχυση με την ακτίνα 'ray'. Τέλος, σε κείμενα με πολλά σημεία στίξης μπορεί επίσης να παρατηρηθεί αύξηση του perplexity από τη μετατροπή των χαρακτήρων σε πεζούς."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Δημιουργία Τυχαίων Προτάσεων από τα Μοντέλα\n",
    "\n",
    "Στο τελευταίο κομμάτι της άσκησης θα πρέπει να δημιουργήσουμε προτάσεις από τα 4 μοντέλα που έχουμε φτιάξει παραπάνω. Για το λόγο αυτό δημιουργούμε δύο functions που θα χρησιμοποιήσουμε για την επίτευξη του παραπάνω στόχου: μία για διγράμματα και μία για τριγράμματα."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στις συναρτήσεις δημιουργούμε ένα νεό dictionary από το οποίο αφαιρούνται όλα τα bigrams ή trigrams που περιέχουν το token \\<UNK> το οποίο σύμφωνα με την εκφώνηση δεν θα πρέπει να εμφανίζεται στις προτάσεις μας. Στη συνέχεια ελέγχουμε αν η αρχική λέξη που ζητά ο χρήστης (start_word) υπάρχει σε κάποιο bigram ή trigram από το οποίο μπορεί να ξεκινήσει πρόταση. Εφόσον αυτό δεν ισχύει, η συνάρτηση τερματίζει και επιστρέφει μήνυμα λάθους.\n",
    "\n",
    "\n",
    "Αν πράγματι υπάρχει πρόταση που μπορεί να ξεκινήσει με αυτή τη λέξη ξεκινά ένα while loop που ελέγχει κάθε φορά πιθανές επόμενες λέξεις και ακολούθως επιλέξει μία λέξη με βάση τα βάρη των bigrams ή trigrams ανάλογα με τη συνάρτηση. Η **if** που υπάρχει στο while loop έχει ως στόχο να αποφύγει τη δημιουργία infinite loop επιλέγοντας ένα bigram ή trigram που δεν μπορεί τελικά να οδηγήσει στο token \\<EOS> για να τερματιστεί η πρόταση.\n",
    "\n",
    "\n",
    "Τέλος, όταν το επόμενο bigram ή trigram που πρέπει να προστεθεί περιέχει το ειδικό token \\<EOS> η πρόταση ολοκληρώνεται και επιστρέφεται στο χρήστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_bigram_sentence(bigrams_dict_input, start_word):\n",
    "    \n",
    "    bigrams_dict = {}\n",
    "    for key, value in bigrams_dict_input.items():\n",
    "        if key[0] != '<UNK>' and key[1] != '<UNK>':\n",
    "            bigrams_dict[key] = value    \n",
    "             \n",
    "    can_generate_sentence = False\n",
    "    for key in bigrams_dict.keys():\n",
    "        if key[0] == '<BOS>' and key[1] == start_word:\n",
    "            can_generate_sentence = True\n",
    "\n",
    "    if can_generate_sentence == False:\n",
    "        print('Bigram not found, impossible to start a sentence to with that word.')\n",
    "        return\n",
    "\n",
    "    sentence = ['<BOS>']\n",
    "    sentence.append(start_word)\n",
    "    current_word = start_word\n",
    "    \n",
    "    while 1:\n",
    "        possible_next_words = []\n",
    "        for word in bigrams_dict.keys():\n",
    "            if word[0] == current_word:\n",
    "                possible_next_words.append((word[1], bigrams_dict[word]))\n",
    "\n",
    "        if len(possible_next_words) == 0:\n",
    "            sentence = ['<BOS>']\n",
    "            sentence.append(start_word)\n",
    "            current_word = start_word\n",
    "            continue\n",
    "\n",
    "        next_word_options = [word[0] for word in possible_next_words]\n",
    "        weights = [word[1] for word in possible_next_words]\n",
    "        next_word = random.choices(next_word_options, weights=weights)[0]\n",
    "\n",
    "        sentence.append(next_word)\n",
    "        current_word = next_word\n",
    "\n",
    "        if next_word == '<EOS>':\n",
    "            break\n",
    "    \n",
    "    return ' '.join(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigram_sentence(trigrams_dict_input, start_word):\n",
    "    \n",
    "    trigrams_dict = {}\n",
    "\n",
    "    for key, value in trigrams_dict_input.items():\n",
    "        if key[0] != '<UNK>' and key[1] != '<UNK>' and key[2] != '<UNK>':\n",
    "            trigrams_dict[key] = value    \n",
    "             \n",
    "    can_generate_sentence = False\n",
    "    for key in trigrams_dict.keys():\n",
    "        if key[0] == '<BOS>' and key[1] == start_word:\n",
    "            can_generate_sentence = True\n",
    "\n",
    "    if can_generate_sentence == False:\n",
    "        print('Trigram not found, impossible to start a sentence to with that word.')\n",
    "        return\n",
    "\n",
    "    sentence = ['<BOS>']\n",
    "    sentence.append(start_word)\n",
    "    \n",
    "    current_word_1 = '<BOS>'\n",
    "    current_word_2 = start_word\n",
    "    \n",
    "    while 1:\n",
    "        possible_next_words = []\n",
    "\n",
    "        for word in trigrams_dict.keys():\n",
    "            if word[0] == current_word_1 and word[1] == current_word_2:\n",
    "                possible_next_words.append((word[2], trigrams_dict[word]))\n",
    "\n",
    "        if len(possible_next_words) == 0:\n",
    "            sentence = ['<BOS>']\n",
    "            sentence.append(start_word)\n",
    "            current_word_1 = '<BOS>'\n",
    "            current_word_2 = start_word\n",
    "            continue\n",
    "\n",
    "        next_word_options = [word[0] for word in possible_next_words]\n",
    "        weights = [word[1] for word in possible_next_words]\n",
    "\n",
    "        next_word = random.choices(next_word_options, weights=weights)[0]\n",
    "\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        current_word_1 = current_word_2\n",
    "        current_word_2 = next_word\n",
    "\n",
    "        if next_word == '<EOS>':\n",
    "            break\n",
    "    \n",
    "    return ' '.join(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 Τυχαίες Προτάσεις με το Μοντέλο Διγραμμάτων με k=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <BOS> I asked , Ohio , said 0 for them for his letter to call to Charles E. Trotter III and foreign assistance and 0 than $ 15,000 *U* plus the Justice Department and are no asbestos will be delivered *-2 only an issue was essentially buy back that *T*-1 Richard Nixon , and chief executive vice president of principal of persons are meant only after UAL stake . <EOS>\n",
      "Sentence 2: <BOS> I 'd have a request and has been found *-2 to put out there 's $ 30 . <EOS>\n",
      "Sentence 3: <BOS> I have wanted *-1 with ringers . * with a credit standing required * joining forces investors , and Mexico Fund Report , most respected floor of sugar , Calif. , Colo. , `` the announcer talks with constitutional authority dropped below a new stadium was one place * for which *T*-1 . <EOS>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    sentence = generate_bigram_sentence(bigram_k001_model, start_word='I')\n",
    "    if sentence == None:\n",
    "        break\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 Τυχαίες Προτάσεις με το Μοντέλο Διγραμμάτων με k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <BOS> A planned acquisition of all '' are real estate and 10-day suspension ; Romanee-Conti , particularly true . <EOS>\n",
      "Sentence 2: <BOS> A lack of all issues , *-1 to an appropriations clause -LRB- priced *-1 much -- products like Contel Corp. 's New England , though probably not students and services that he has become the government payments with ringers 0 *T*-2 . <EOS>\n",
      "Sentence 3: <BOS> A couple of the region 's program , it : An official . <EOS>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    sentence = generate_bigram_sentence(bigram_k1_model, start_word='A')\n",
    "    if sentence == None:\n",
    "        break\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 Τυχαίες Προτάσεις με το Μοντέλο Τριγραμμάτων με k=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <BOS> I say `` * providing 0 the ban on virtually all of us -LRB- but now , however . <EOS>\n",
      "Sentence 2: <BOS> I believe in the Chicago Mercantile Exchange , a further 25 % in 1991 to 7.458 % . <EOS>\n",
      "Sentence 3: <BOS> I get the answers to these people . <EOS>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    sentence = generate_trigram_sentence(trigram_k001_model, start_word='I')\n",
    "    if sentence == None:\n",
    "        break\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 Τυχαίες Προτάσεις με το Μοντέλο Τριγραμμάτων με k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <BOS> A year ago , I 'd have a real bad day , may be low , 8 3\\/4 % to $ 90 to $ 8.5 million *U* last year . <EOS>\n",
      "Sentence 2: <BOS> A major concern about such practices , according to Ms. Poore , the lowest -- a product for upscale professionals . <EOS>\n",
      "Sentence 3: <BOS> A bank spokeswoman also declined *-1 to be covered *-1 . <EOS>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    sentence = generate_trigram_sentence(trigram_k1_model, start_word='A')\n",
    "    if sentence == None:\n",
    "        break\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Τελικές Παρατηρήσεις\n",
    "\n",
    "Όπως φαίνεται από τις τυχαίες προτάσεις που δημιουργούνται, γενικά τα μοντέλα τριγραμμάτων δημιουργούν πιο ανθρώπινες προτάσεις που βρίσκονται πιο κοντά στη φυσική γλώσσα. Ωστόσο, παρατηρούμε ότι αρκετές προτάσεις περιέχουν διάφορα σύμβολα και σημεία στίξης, γεγονός που όπως φαίνεται δημιουργεί θόρυβο και τελικά μειώνει την αποτελεσματικότητα του μοντέλου. Το γεγονός αυτό μπορεί να εξηγήσει ίσως γιατί το perplexity των μοντέλων διγραμμάτων μειώθηκε μετά την μετατροπή των χαρακτήρων σε πεζούς."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45187ec696c755c7517b5d53ebdf5671ced61caad9fc733db8bb80f06c3eeb1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
