{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#\n# Named-entity recognition using BERT\n# Dataset: https://www.kaggle.com/datasets/alaakhaled/conll003-englishversion\n#\n\n# dependencies\nimport torch\nimport torch.optim as optim \nfrom torchtext.vocab import build_vocab_from_iterator\nfrom transformers import BertForTokenClassification, BertTokenizerFast\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\nimport tqdm\ntqdmn = tqdm.notebook.tqdm","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:36.615006Z","iopub.execute_input":"2023-06-03T08:21:36.615364Z","iopub.status.idle":"2023-06-03T08:21:36.621925Z","shell.execute_reply.started":"2023-06-03T08:21:36.615336Z","shell.execute_reply":"2023-06-03T08:21:36.620938Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# hyper-parameters\nEPOCHS = 3\nBATCH_SIZE = 8\nLR = 1e-5","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:36.623651Z","iopub.execute_input":"2023-06-03T08:21:36.623996Z","iopub.status.idle":"2023-06-03T08:21:36.636136Z","shell.execute_reply.started":"2023-06-03T08:21:36.623962Z","shell.execute_reply":"2023-06-03T08:21:36.635096Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# the path of the data files\nbase_path = '/kaggle/input/conll003-englishversion/'\n\n# use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:36.637576Z","iopub.execute_input":"2023-06-03T08:21:36.638088Z","iopub.status.idle":"2023-06-03T08:21:36.646174Z","shell.execute_reply.started":"2023-06-03T08:21:36.638055Z","shell.execute_reply":"2023-06-03T08:21:36.645017Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# read the data files\ndef load_sentences(filepath):\n\n    sentences = []\n    tokens = []\n    pos_tags = []\n    chunk_tags = []\n    ner_tags = []\n\n    with open(filepath, 'r') as f:\n        \n        for line in f.readlines():\n            \n            if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n                if len(tokens) > 0:\n                    sentences.append({'tokens': tokens, 'pos_tags': pos_tags, 'chunk_tags': chunk_tags, 'ner_tags': ner_tags})\n                    tokens = []\n                    pos_tags = []\n                    chunk_tags = []\n                    ner_tags = []\n            else:\n                l = line.split(' ')\n                tokens.append(l[0])\n                pos_tags.append(l[1])\n                chunk_tags.append(l[2])\n                ner_tags.append(l[3].strip('\\n'))\n    \n    return sentences","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:36.647563Z","iopub.execute_input":"2023-06-03T08:21:36.648505Z","iopub.status.idle":"2023-06-03T08:21:36.658617Z","shell.execute_reply.started":"2023-06-03T08:21:36.648472Z","shell.execute_reply":"2023-06-03T08:21:36.657636Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print('loading data')\ntrain_sentences = load_sentences(base_path + 'train.txt')\ntest_sentences = load_sentences(base_path + 'test.txt')\nvalid_sentences = load_sentences(base_path + 'valid.txt')","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:36.661852Z","iopub.execute_input":"2023-06-03T08:21:36.662198Z","iopub.status.idle":"2023-06-03T08:21:37.480217Z","shell.execute_reply.started":"2023-06-03T08:21:36.662174Z","shell.execute_reply":"2023-06-03T08:21:37.479280Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"loading data\n","output_type":"stream"}]},{"cell_type":"code","source":"# build tagset and tag ids\ntags = [sentence['ner_tags'] for sentence in train_sentences]\ntagmap = build_vocab_from_iterator(tags)\ntagset = set([item for sublist in tags for item in sublist])\nprint('Tagset size:',len(tagset))","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:37.481621Z","iopub.execute_input":"2023-06-03T08:21:37.482053Z","iopub.status.idle":"2023-06-03T08:21:37.624679Z","shell.execute_reply.started":"2023-06-03T08:21:37.482018Z","shell.execute_reply":"2023-06-03T08:21:37.623662Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Tagset size: 9\n","output_type":"stream"}]},{"cell_type":"code","source":"# load BERT tokenizer\nbert_version = 'bert-base-uncased'\ntokenizer = BertTokenizerFast.from_pretrained(bert_version)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:37.626357Z","iopub.execute_input":"2023-06-03T08:21:37.626736Z","iopub.status.idle":"2023-06-03T08:21:37.804219Z","shell.execute_reply.started":"2023-06-03T08:21:37.626701Z","shell.execute_reply":"2023-06-03T08:21:37.803291Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# map tokens and tags to token ids and label ids\ndef align_label(tokens, labels):\n\n    word_ids = tokens.word_ids()\n    previous_word_idx = None\n    label_ids = []\n    for word_idx in word_ids:\n        if word_idx is None:\n            label_ids.append(-100)\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(tagmap[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:37.805517Z","iopub.execute_input":"2023-06-03T08:21:37.805875Z","iopub.status.idle":"2023-06-03T08:21:37.813569Z","shell.execute_reply.started":"2023-06-03T08:21:37.805843Z","shell.execute_reply":"2023-06-03T08:21:37.812620Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def encode(sentence):\n    encodings = tokenizer(sentence['tokens'], truncation=True, padding='max_length', is_split_into_words=True)\n    labels = align_label(encodings, sentence['ner_tags'])\n    return { 'input_ids': torch.LongTensor(encodings.input_ids), 'attention_mask': torch.LongTensor(encodings.attention_mask), 'labels': torch.LongTensor(labels) }","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:37.815082Z","iopub.execute_input":"2023-06-03T08:21:37.815722Z","iopub.status.idle":"2023-06-03T08:21:37.822943Z","shell.execute_reply.started":"2023-06-03T08:21:37.815689Z","shell.execute_reply":"2023-06-03T08:21:37.822045Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print('encoding data')\ntrain_dataset = [encode(sentence) for sentence in train_sentences]\nvalid_dataset = [encode(sentence) for sentence in valid_sentences]\ntest_dataset = [encode(sentence) for sentence in test_sentences]","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:37.824259Z","iopub.execute_input":"2023-06-03T08:21:37.824608Z","iopub.status.idle":"2023-06-03T08:21:51.286800Z","shell.execute_reply.started":"2023-06-03T08:21:37.824562Z","shell.execute_reply":"2023-06-03T08:21:51.285646Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"encoding data\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize the model including a classification layer with num_labels classes\nprint('initializing the model')\nmodel = BertForTokenClassification.from_pretrained(bert_version, num_labels=len(tagset))\n\n# By setting requires_grad to False we make sure that the parameters will not be changed (aka remain frozen) during the training process\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n\nmodel.to(device)\n\n# Only the classifier (final) level parameters will be changed by the optimizer\noptimizer = optim.AdamW(params=model.classifier.parameters(), lr=LR)\n\nfrozen_params = 0\nnon_frozen_params = 0\n\nfor param in model.parameters():\n    if param.requires_grad:\n        non_frozen_params += param.numel()\n    else:\n        frozen_params += param.numel()\n\nprint(\"Number of frozen parameters: \", frozen_params)\nprint(\"Number of non-frozen parameters: \", non_frozen_params)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:51.288492Z","iopub.execute_input":"2023-06-03T08:21:51.288867Z","iopub.status.idle":"2023-06-03T08:21:52.557609Z","shell.execute_reply.started":"2023-06-03T08:21:51.288834Z","shell.execute_reply":"2023-06-03T08:21:52.556531Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"initializing the model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Number of frozen parameters:  108891648\nNumber of non-frozen parameters:  6921\n","output_type":"stream"}]},{"cell_type":"code","source":"# prepare batches of data\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:52.559211Z","iopub.execute_input":"2023-06-03T08:21:52.559634Z","iopub.status.idle":"2023-06-03T08:21:52.632554Z","shell.execute_reply.started":"2023-06-03T08:21:52.559583Z","shell.execute_reply":"2023-06-03T08:21:52.631709Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# evaluate the performance of the model\ndef EvaluateModel(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        Y_actual, Y_preds = [],[]\n        for i, batch in enumerate(tqdmn(data_loader)):\n            # move the batch tensors to the same device as the model\n            batch = { k: v.to(device) for k, v in batch.items() }\n            # send 'input_ids', 'attention_mask' and 'labels' to the model\n            outputs = model(**batch)\n            # iterate through the examples\n            for idx, _ in enumerate(batch['labels']):\n                # get the true values\n                true_values_all = batch['labels'][idx]\n                true_values = true_values_all[true_values_all != -100]\n                # get the predicted values\n                pred_values = torch.argmax(outputs[1], dim=2)[idx]\n                pred_values = pred_values[true_values_all != -100]\n                # update the lists of true answers and predictions\n                Y_actual.append(true_values)\n                Y_preds.append(pred_values)\n        Y_actual = torch.cat(Y_actual)\n        Y_preds = torch.cat(Y_preds)\n    # Return list of actual labels, predicted labels \n    return Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:52.642705Z","iopub.execute_input":"2023-06-03T08:21:52.642982Z","iopub.status.idle":"2023-06-03T08:21:52.652087Z","shell.execute_reply.started":"2023-06-03T08:21:52.642958Z","shell.execute_reply":"2023-06-03T08:21:52.651039Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# train the model\nprint('training the model')\nfor epoch in tqdmn(range(EPOCHS)):\n    model.train()\n    print('epoch',epoch+1)\n    # iterate through each batch of the train data\n    for i, batch in enumerate(tqdmn(train_loader)):\n        # move the batch tensors to the same device as the model\n        batch = { k: v.to(device) for k, v in batch.items() }\n        # send 'input_ids', 'attention_mask' and 'labels' to the model\n        outputs = model(**batch)\n        loss = outputs[0]\n        # set the gradients to zero\n        optimizer.zero_grad()\n        # propagate the loss backwards\n        loss.backward()\n        # update the model weights\n        optimizer.step()\n    # calculate performence on validation set\n    Y_actual, Y_preds = EvaluateModel(model,valid_loader)\n    print(\"\\nValidation Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n    print(\"\\nValidation Macro-Accuracy : {:.3f}\".format(balanced_accuracy_score(Y_actual, Y_preds)))","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:21:52.653562Z","iopub.execute_input":"2023-06-03T08:21:52.654077Z","iopub.status.idle":"2023-06-03T08:36:57.530461Z","shell.execute_reply.started":"2023-06-03T08:21:52.654046Z","shell.execute_reply":"2023-06-03T08:36:57.529429Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"training the model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7021e18038d042b7beeb9af69c436ce9"}},"metadata":{}},{"name":"stdout","text":"epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0962cc1ee21244908b12d5d042a0031a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fec6f86a5ea4fd69212a514834e6612"}},"metadata":{}},{"name":"stdout","text":"\nValidation Accuracy : 0.831\n\nValidation Macro-Accuracy : 0.117\nepoch 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e29794d874f4ffb9875ff7c3d789caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc56381ab4c64fe6b526c2bbd5e36f67"}},"metadata":{}},{"name":"stdout","text":"\nValidation Accuracy : 0.835\n\nValidation Macro-Accuracy : 0.129\nepoch 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82902991d5bd4410ba750f7799c059e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757e6914310f43c286d5eb1d34cfcaad"}},"metadata":{}},{"name":"stdout","text":"\nValidation Accuracy : 0.851\n\nValidation Macro-Accuracy : 0.183\n","output_type":"stream"}]},{"cell_type":"code","source":"print('applying the model to the test set')\n# apply the trained model to the test set\nY_actual, Y_preds = EvaluateModel(model,test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:36:57.531860Z","iopub.execute_input":"2023-06-03T08:36:57.532291Z","iopub.status.idle":"2023-06-03T08:37:56.276191Z","shell.execute_reply.started":"2023-06-03T08:36:57.532256Z","shell.execute_reply":"2023-06-03T08:37:56.275358Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"applying the model to the test set\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/432 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeac7b99407542c3aa788a8aaca2876f"}},"metadata":{}}]},{"cell_type":"code","source":"print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\nprint(\"\\nTest Macro-Accuracy : {:.3f}\".format(balanced_accuracy_score(Y_actual, Y_preds)))\nprint(\"\\nClassification Report : \")\nprint(classification_report(Y_actual, Y_preds,labels = tagmap(tagmap.get_itos()), target_names = tagmap.get_itos(), zero_division = 0))","metadata":{"execution":{"iopub.status.busy":"2023-06-03T08:37:56.280146Z","iopub.execute_input":"2023-06-03T08:37:56.282251Z","iopub.status.idle":"2023-06-03T08:37:56.368901Z","shell.execute_reply.started":"2023-06-03T08:37:56.282216Z","shell.execute_reply":"2023-06-03T08:37:56.367914Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\nTest Accuracy : 0.848\n\nTest Macro-Accuracy : 0.196\n\nClassification Report : \n              precision    recall  f1-score   support\n\n           O       0.85      1.00      0.92     38323\n       B-LOC       0.81      0.28      0.42      1668\n       B-PER       0.89      0.16      0.27      1617\n       B-ORG       0.56      0.22      0.31      1661\n       I-PER       0.90      0.11      0.19      1156\n       I-ORG       0.00      0.00      0.00       835\n      B-MISC       0.00      0.00      0.00       702\n       I-LOC       0.00      0.00      0.00       257\n      I-MISC       0.00      0.00      0.00       216\n\n    accuracy                           0.85     46435\n   macro avg       0.45      0.20      0.23     46435\nweighted avg       0.81      0.85      0.80     46435\n\n","output_type":"stream"}]}]}