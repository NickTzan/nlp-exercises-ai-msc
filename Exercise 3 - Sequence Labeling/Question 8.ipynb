{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"First of all we have to import the roberta pretrained model from the transformers library.","metadata":{}},{"cell_type":"code","source":"#\n# Named-entity recognition using BERT\n# Dataset: https://www.kaggle.com/datasets/alaakhaled/conll003-englishversion\n#\n\n# dependencies\nimport torch\nimport torch.optim as optim \nfrom torchtext.vocab import build_vocab_from_iterator\nfrom transformers import RobertaForTokenClassification, RobertaTokenizerFast\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\nimport tqdm\ntqdmn = tqdm.notebook.tqdm","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:28.140697Z","iopub.execute_input":"2023-06-03T11:48:28.141125Z","iopub.status.idle":"2023-06-03T11:48:28.156447Z","shell.execute_reply.started":"2023-06-03T11:48:28.141084Z","shell.execute_reply":"2023-06-03T11:48:28.155544Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# hyper-parameters\nEPOCHS = 3\nBATCH_SIZE = 8\nLR = 1e-5","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:28.158312Z","iopub.execute_input":"2023-06-03T11:48:28.159186Z","iopub.status.idle":"2023-06-03T11:48:28.165990Z","shell.execute_reply.started":"2023-06-03T11:48:28.159160Z","shell.execute_reply":"2023-06-03T11:48:28.164853Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# the path of the data files\nbase_path = '/kaggle/input/conll003-englishversion/'\n\n# use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:28.167431Z","iopub.execute_input":"2023-06-03T11:48:28.168187Z","iopub.status.idle":"2023-06-03T11:48:28.173902Z","shell.execute_reply.started":"2023-06-03T11:48:28.168131Z","shell.execute_reply":"2023-06-03T11:48:28.172848Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# read the data files\ndef load_sentences(filepath):\n\n    sentences = []\n    tokens = []\n    pos_tags = []\n    chunk_tags = []\n    ner_tags = []\n\n    with open(filepath, 'r') as f:\n        \n        for line in f.readlines():\n            \n            if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n                if len(tokens) > 0:\n                    sentences.append({'tokens': tokens, 'pos_tags': pos_tags, 'chunk_tags': chunk_tags, 'ner_tags': ner_tags})\n                    tokens = []\n                    pos_tags = []\n                    chunk_tags = []\n                    ner_tags = []\n            else:\n                l = line.split(' ')\n                tokens.append(l[0])\n                pos_tags.append(l[1])\n                chunk_tags.append(l[2])\n                ner_tags.append(l[3].strip('\\n'))\n    \n    return sentences","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:28.176864Z","iopub.execute_input":"2023-06-03T11:48:28.177704Z","iopub.status.idle":"2023-06-03T11:48:28.187507Z","shell.execute_reply.started":"2023-06-03T11:48:28.177673Z","shell.execute_reply":"2023-06-03T11:48:28.186583Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"print('loading data')\ntrain_sentences = load_sentences(base_path + 'train.txt')\ntest_sentences = load_sentences(base_path + 'test.txt')\nvalid_sentences = load_sentences(base_path + 'valid.txt')","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:28.188690Z","iopub.execute_input":"2023-06-03T11:48:28.189380Z","iopub.status.idle":"2023-06-03T11:48:29.173878Z","shell.execute_reply.started":"2023-06-03T11:48:28.189349Z","shell.execute_reply":"2023-06-03T11:48:29.172952Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"loading data\n","output_type":"stream"}]},{"cell_type":"code","source":"# build tagset and tag ids\ntags = [sentence['ner_tags'] for sentence in train_sentences]\ntagmap = build_vocab_from_iterator(tags)\ntagset = set([item for sublist in tags for item in sublist])\nprint('Tagset size:',len(tagset))","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:29.175213Z","iopub.execute_input":"2023-06-03T11:48:29.175550Z","iopub.status.idle":"2023-06-03T11:48:29.325552Z","shell.execute_reply.started":"2023-06-03T11:48:29.175518Z","shell.execute_reply":"2023-06-03T11:48:29.324617Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"Tagset size: 9\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The next step is to load the roberta tokenizer.","metadata":{}},{"cell_type":"code","source":"# load the roberta tokenizer\nroberta_version = 'roberta-base'\ntokenizer = RobertaTokenizerFast.from_pretrained(roberta_version, add_prefix_space=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:29.327236Z","iopub.execute_input":"2023-06-03T11:48:29.327604Z","iopub.status.idle":"2023-06-03T11:48:31.807288Z","shell.execute_reply.started":"2023-06-03T11:48:29.327556Z","shell.execute_reply":"2023-06-03T11:48:31.806362Z"},"trusted":true},"execution_count":112,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a95dd65d8a6e460d8ef41bb94b489ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a1f1f016268453f9e51e6a8d9cd73d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d7ecfdf1e840d7b454999ead74ac86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"114cce039dee4ee7b52ed0eca95dc68c"}},"metadata":{}}]},{"cell_type":"code","source":"# map tokens and tags to token ids and label ids\ndef align_label(tokens, labels):\n\n    word_ids = tokens.word_ids()\n    previous_word_idx = None\n    label_ids = []\n    for word_idx in word_ids:\n        if word_idx is None:\n            label_ids.append(-100)\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(tagmap[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:31.812904Z","iopub.execute_input":"2023-06-03T11:48:31.813202Z","iopub.status.idle":"2023-06-03T11:48:31.823116Z","shell.execute_reply.started":"2023-06-03T11:48:31.813176Z","shell.execute_reply":"2023-06-03T11:48:31.821796Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"def encode(sentence):\n    encodings = tokenizer(sentence['tokens'], truncation=True, padding='max_length', is_split_into_words=True)\n    labels = align_label(encodings, sentence['ner_tags'])\n    return { 'input_ids': torch.LongTensor(encodings.input_ids), 'attention_mask': torch.LongTensor(encodings.attention_mask), 'labels': torch.LongTensor(labels) }","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:31.824537Z","iopub.execute_input":"2023-06-03T11:48:31.825148Z","iopub.status.idle":"2023-06-03T11:48:31.832815Z","shell.execute_reply.started":"2023-06-03T11:48:31.825114Z","shell.execute_reply":"2023-06-03T11:48:31.831871Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"print('encoding data')\ntrain_dataset = [encode(sentence) for sentence in train_sentences]\nvalid_dataset = [encode(sentence) for sentence in valid_sentences]\ntest_dataset = [encode(sentence) for sentence in test_sentences]","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:31.836260Z","iopub.execute_input":"2023-06-03T11:48:31.836632Z","iopub.status.idle":"2023-06-03T11:48:45.839687Z","shell.execute_reply.started":"2023-06-03T11:48:31.836599Z","shell.execute_reply":"2023-06-03T11:48:45.838739Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"encoding data\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize the model including a classification layer with num_labels classes\nprint('initializing the model')\nmodel = RobertaForTokenClassification.from_pretrained(roberta_version, num_labels=len(tagset))\nmodel.to(device)\noptimizer = optim.AdamW(params=model.parameters(), lr=LR)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:45.841281Z","iopub.execute_input":"2023-06-03T11:48:45.841676Z","iopub.status.idle":"2023-06-03T11:48:48.722941Z","shell.execute_reply.started":"2023-06-03T11:48:45.841640Z","shell.execute_reply":"2023-06-03T11:48:48.721997Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"initializing the model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c42a3dcb9447d183cdb8fd5ad3df49"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# prepare batches of data\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:48.724775Z","iopub.execute_input":"2023-06-03T11:48:48.725199Z","iopub.status.idle":"2023-06-03T11:48:48.800113Z","shell.execute_reply.started":"2023-06-03T11:48:48.725165Z","shell.execute_reply":"2023-06-03T11:48:48.799181Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# evaluate the performance of the model\ndef EvaluateModel(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        Y_actual, Y_preds = [],[]\n        for i, batch in enumerate(tqdmn(data_loader)):\n            # move the batch tensors to the same device as the model\n            batch = { k: v.to(device) for k, v in batch.items() }\n            # send 'input_ids', 'attention_mask' and 'labels' to the model\n            outputs = model(**batch)\n            # iterate through the examples\n            for idx, _ in enumerate(batch['labels']):\n                # get the true values\n                true_values_all = batch['labels'][idx]\n                true_values = true_values_all[true_values_all != -100]\n                # get the predicted values\n                pred_values = torch.argmax(outputs[1], dim=2)[idx]\n                pred_values = pred_values[true_values_all != -100]\n                # update the lists of true answers and predictions\n                Y_actual.append(true_values)\n                Y_preds.append(pred_values)\n        Y_actual = torch.cat(Y_actual)\n        Y_preds = torch.cat(Y_preds)\n    # Return list of actual labels, predicted labels \n    return Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:48.805769Z","iopub.execute_input":"2023-06-03T11:48:48.806074Z","iopub.status.idle":"2023-06-03T11:48:48.814304Z","shell.execute_reply.started":"2023-06-03T11:48:48.806048Z","shell.execute_reply":"2023-06-03T11:48:48.813230Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# train the model\nprint('training the model')\nfor epoch in tqdmn(range(EPOCHS)):\n    model.train()\n    print('epoch',epoch+1)\n    # iterate through each batch of the train data\n    for i, batch in enumerate(tqdmn(train_loader)):\n        # move the batch tensors to the same device as the model\n        batch = { k: v.to(device) for k, v in batch.items() }\n        # send 'input_ids', 'attention_mask' and 'labels' to the model\n        outputs = model(**batch)\n        loss = outputs[0]\n        # set the gradients to zero\n        optimizer.zero_grad()\n        # propagate the loss backwards\n        loss.backward()\n        # update the model weights\n        optimizer.step()\n    # calculate performence on validation set\n    Y_actual, Y_preds = EvaluateModel(model,valid_loader)\n    print(\"\\nValidation Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n    print(\"\\nValidation Macro-Accuracy : {:.3f}\".format(balanced_accuracy_score(Y_actual, Y_preds)))","metadata":{"execution":{"iopub.status.busy":"2023-06-03T11:48:48.815823Z","iopub.execute_input":"2023-06-03T11:48:48.816803Z","iopub.status.idle":"2023-06-03T12:29:13.703539Z","shell.execute_reply.started":"2023-06-03T11:48:48.816770Z","shell.execute_reply":"2023-06-03T12:29:13.702478Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"training the model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ce4e6c4e274e91aaf252b6afcc010d"}},"metadata":{}},{"name":"stdout","text":"epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52228ea7c3794677943f5a6cca40b50b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6350636ea42048e6b266f76b97fb3072"}},"metadata":{}},{"name":"stdout","text":"\nValidation Accuracy : 0.990\n\nValidation Macro-Accuracy : 0.947\nepoch 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf6c124dbcd42afb3bbbed85795211e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f909a6578be54873a48341783f2196b2"}},"metadata":{}},{"name":"stdout","text":"\nValidation Accuracy : 0.991\n\nValidation Macro-Accuracy : 0.950\nepoch 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1756 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8e56891d044b45969dc025b1f339a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/407 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82ca44d36d1d4b1185b65ae8bf37f6cb"}},"metadata":{}},{"name":"stdout","text":"\nValidation Accuracy : 0.993\n\nValidation Macro-Accuracy : 0.961\n","output_type":"stream"}]},{"cell_type":"code","source":"print('applying the model to the test set')\n# apply the trained model to the test set\nY_actual, Y_preds = EvaluateModel(model,test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T12:29:13.705287Z","iopub.execute_input":"2023-06-03T12:29:13.706251Z","iopub.status.idle":"2023-06-03T12:30:12.793924Z","shell.execute_reply.started":"2023-06-03T12:29:13.706215Z","shell.execute_reply":"2023-06-03T12:30:12.793032Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"applying the model to the test set\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/432 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7641967872748b1ab1454e58af099e3"}},"metadata":{}}]},{"cell_type":"code","source":"print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\nprint(\"\\nTest Macro-Accuracy : {:.3f}\".format(balanced_accuracy_score(Y_actual, Y_preds)))\nprint(\"\\nClassification Report : \")\nprint(classification_report(Y_actual, Y_preds,labels = tagmap(tagmap.get_itos()), target_names = tagmap.get_itos(), zero_division = 0))","metadata":{"execution":{"iopub.status.busy":"2023-06-03T12:30:12.795271Z","iopub.execute_input":"2023-06-03T12:30:12.795931Z","iopub.status.idle":"2023-06-03T12:30:12.863670Z","shell.execute_reply.started":"2023-06-03T12:30:12.795896Z","shell.execute_reply":"2023-06-03T12:30:12.862648Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"\nTest Accuracy : 0.984\n\nTest Macro-Accuracy : 0.926\n\nClassification Report : \n              precision    recall  f1-score   support\n\n           O       1.00      0.99      1.00     38323\n       B-LOC       0.95      0.94      0.94      1668\n       B-PER       0.98      0.96      0.97      1617\n       B-ORG       0.91      0.93      0.92      1661\n       I-PER       0.99      0.99      0.99      1156\n       I-ORG       0.87      0.93      0.90       835\n      B-MISC       0.82      0.87      0.84       702\n       I-LOC       0.87      0.93      0.90       257\n      I-MISC       0.68      0.79      0.73       216\n\n    accuracy                           0.98     46435\n   macro avg       0.89      0.93      0.91     46435\nweighted avg       0.98      0.98      0.98     46435\n\n","output_type":"stream"}]}]}